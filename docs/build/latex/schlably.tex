%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Table of Contents}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{2}



\title{Schlably}
\date{Dec 16, 2022}
\release{0.0.1}
\author{AlphaMES\sphinxhyphen{}Team}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxAtStartPar
\sphinxstylestrong{Schlably \sphinxhyphen{} A Python\sphinxhyphen{}Based Scheduling Laboratory for Reinforcement Learning Solutions}

\sphinxAtStartPar
Schlably is a Python\sphinxhyphen{}Based framework for experiments on scheduling problems with Deep Reinforcement Learning (DRL).
It features an extendable gym environment and DRL\sphinxhyphen{}Agents along with code for data generation, training and testing.

\sphinxAtStartPar
Schlably was developed such that modules may be used as they are, but also may be customized to fit the needs of the user.
While the framework works out of the box and can be adjusted through config files, some changes are intentionally only possible through changes to the code.
We believe that this makes it easier to apply small changes without having to deal with complex multi\sphinxhyphen{}level inheritances.

\sphinxAtStartPar
Github: url

\sphinxAtStartPar
Paper: arxiv link


\chapter{Main Features}
\label{\detokenize{index:main-features}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Reinforcement Learning

\item {} 
\sphinxAtStartPar
Clean Code

\item {} 
\sphinxAtStartPar
…

\end{itemize}


\chapter{User Guide}
\label{\detokenize{index:user-guide}}

\section{Installation}
\label{\detokenize{installation:installation}}\label{\detokenize{installation::doc}}
\sphinxAtStartPar
To install all necessary packages run

\begin{sphinxVerbatim}[commandchars=\\\{\}]
pip install \PYGZhy{}r requirements.txt
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Schlably was developed in Python 3.10. It may work with higher versions of Python 3, but this is not guaranteed.
\end{sphinxadmonition}

\sphinxAtStartPar
If you want to use \sphinxhref{https://wandb.ai/site}{Weights\&Biases} (wandb) for logging, which we highly recommend,
you need to (create and) login with your account. Open a terminal and run:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
wandb login
\end{sphinxVerbatim}


\section{Quickstart}
\label{\detokenize{quickstart:quickstart}}\label{\detokenize{quickstart::doc}}

\subsection{Data Generation}
\label{\detokenize{quickstart:data-generation}}
\sphinxAtStartPar
To create your own data, or more precisely, instances of a scheduling problem, proceed as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Create a custom data generation configuration from one of the configs listed in \sphinxstyleemphasis{config/data\_generation/fjssp} or \sphinxstyleemphasis{config/data\_generation/jssp} (e.g. change number of machines, tasks, tools, runtimes etc.) to specify the generated instances.

\item {} 
\sphinxAtStartPar
Run

\begin{sphinxVerbatim}[commandchars=\\\{\}]
python \PYGZhy{}m src.data\PYGZus{}generator.instance\PYGZus{}factory \PYGZhy{}fp data\PYGZus{}generation/jssp/\PYGZlt{}your\PYGZus{}data\PYGZus{}generation\PYGZus{}config\PYGZgt{}.yaml
\end{sphinxVerbatim}

\end{enumerate}


\subsection{Training}
\label{\detokenize{quickstart:training}}
\sphinxAtStartPar
To train your own model, proceed as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
To train a model, you need to specify a training config, such as the ones already included in \sphinxstyleemphasis{config/training}. Note that different agents have different configs because the come with different algorithm parameters. You can customize the config to your needs, e.g. change the number of episodes, the learning rate, the batch size etc.
We are using weights \& biases (wandb) to track our results.
If you want to track your results online, create your project at wandb.ai and set config parameter wandb\_mode to 1 (offline tracking) or 2 (online tracking)
and specify \sphinxstyleemphasis{wandb\_project} parameter in config file and \sphinxstyleemphasis{wandb\_entity} constant in the \sphinxstyleemphasis{logger.py} file.

\item {} 
\sphinxAtStartPar
Run

\begin{sphinxVerbatim}[commandchars=\\\{\}]
python \PYGZhy{}m src.agents.train \PYGZhy{}fp training/ppo/\PYGZlt{}your\PYGZus{}training\PYGZus{}config\PYGZgt{}.yaml
\end{sphinxVerbatim}

\end{enumerate}

\sphinxAtStartPar
Immediately after training the model will be tested and benchmarked against all heuristics included in the TEST\_HEURISTICS constant located in \sphinxstyleemphasis{src/agents/test.py}.
The trained model can be accessed via the experiment\_save\_path and saved\_model\_name you specified in the training config.


\subsection{Testing}
\label{\detokenize{quickstart:testing}}
\sphinxAtStartPar
As aforementioned, calling \sphinxcode{\sphinxupquote{train.py}} automatically tests the model once training is complete. If you want to test a certain model again, or benchmark it against other heuristics, proceed as follows:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
As in training, you need to point to a testing config file like the ones provided in \sphinxstyleemphasis{config/testing}.  You may change entries according to your needs.
We provide a pre\sphinxhyphen{}trained PPO model. Thus, creating a config in \sphinxstyleemphasis{config/testing/ppo} and assigning \sphinxstyleemphasis{example\_ppo\_agent} to \sphinxstyleemphasis{saved\_model\_name} allows you to test without training first.

\item {} 
\sphinxAtStartPar
Run

\begin{sphinxVerbatim}[commandchars=\\\{\}]
python \PYGZhy{}m src.agents.test \PYGZhy{}fp testing/ppo/\PYGZlt{}your\PYGZus{}testing\PYGZus{}config\PYGZgt{}.yaml
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Optionally, you may use the parameter \textendash{}plot\sphinxhyphen{}ganttchart to plot the test results.

\end{enumerate}

\sphinxAtStartPar
We have pre\sphinxhyphen{}implemented many common priority dispatching rules, such as Shortest Processing Time first, and a flexible optimal solver.


\subsection{Advanced config handling}
\label{\detokenize{quickstart:advanced-config-handling}}
\sphinxAtStartPar
On the one hand running data\_generation, training or testing by specifying a config file path offers an easy and comfortable way to use this framework, but on the other hand it might seem a bit restrictive.
Therefore, there is the possibility to start all three files by passing a config dictionary to their main functions.
This comes in handy, if you need to loop across multiple configs or if you want to change single parameters without saving new config files.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Please check out the tutorials for further information on how to use the framework.
\end{sphinxadmonition}


\section{Tutorials}
\label{\detokenize{tutorials:tutorials}}\label{\detokenize{tutorials::doc}}

\subsection{Custom Problem Settings}
\label{\detokenize{custom_problem_setting:custom-problem-settings}}\label{\detokenize{custom_problem_setting::doc}}
\sphinxAtStartPar
To illustrate a typical use case, we consider a scenario in which an ML\sphinxhyphen{}
engineer wants to compare the learning behavior of two PPO agents.

\sphinxAtStartPar
Agent one:
* trained on 6x6 JSSP instances
* optimizes for the makespan
* reward function: change in makespan (makespan(t\sphinxhyphen{}1) \sphinxhyphen{} makespan(t))

\sphinxAtStartPar
Agent two:
* trained on 3x4 tool constrained JSSP instances
* optimizes for the makespan
* reward function: change in makespan (makespan(t\sphinxhyphen{}1) \sphinxhyphen{} makespan(t))
\begin{equation*}
\begin{split}f(z) = \left\{ \begin{array}{rcl}
\overline{\overline{z^2}+\cos z} & \mbox{for}
& |z|<3 \\ 0 & \mbox{for} & 3\leq|z|\leq5 \\
\sin\overline{z} & \mbox{for} & |z|>5
\end{array}\right.\end{split}
\end{equation*}

\subsection{Looping over Configuration Parameters}
\label{\detokenize{looping_over_config_params:looping-over-configuration-parameters}}\label{\detokenize{looping_over_config_params::doc}}
\sphinxAtStartPar
This tutorial comes in handy, if you need to loop across multiple configs or if you want to change single parameters
without saving new config files.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Import the main function from agents.train to your script

\item {} 
\sphinxAtStartPar
Load a config or specify an entire config

\item {} 
\sphinxAtStartPar
Loop over the parameter you want to change and update your config before starting a training

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} import training main function}
\PYG{k+kn}{from} \PYG{n+nn}{agents}\PYG{n+nn}{.}\PYG{n+nn}{train} \PYG{k+kn}{import} \PYG{n}{main} \PYG{k}{as} \PYG{n}{training\PYGZus{}main}

\PYG{c+c1}{\PYGZsh{} load a default config}
\PYG{n}{train\PYGZus{}default\PYGZus{}config} \PYG{o}{=} \PYG{n}{ConfigHandler}\PYG{o}{.}\PYG{n}{get\PYGZus{}config}\PYG{p}{(}\PYG{n}{DEFAULT\PYGZus{}TRAINING\PYGZus{}FILE}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} specify your seeds}
\PYG{n}{seeds} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1455}\PYG{p}{,} \PYG{l+m+mi}{2327}\PYG{p}{,} \PYG{l+m+mi}{7776}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} loop}
\PYG{k}{for} \PYG{n}{seed} \PYG{o+ow}{in} \PYG{n}{seeds}\PYG{p}{:}
   \PYG{n}{train\PYGZus{}default\PYGZus{}config}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{seed}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

   \PYG{c+c1}{\PYGZsh{} start training}
   \PYG{n}{training\PYGZus{}main}\PYG{p}{(}\PYG{n}{external\PYGZus{}config}\PYG{o}{=}\PYG{n}{train\PYGZus{}default\PYGZus{}config}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
If you want to use the functionality of sweeps in weights and biases, check out the designated tutorial!
\end{sphinxadmonition}


\subsection{Weights \& Biases sweeps}
\label{\detokenize{weights_and_biases_sweeps:weights-biases-sweeps}}\label{\detokenize{weights_and_biases_sweeps::doc}}
\sphinxAtStartPar
Weights \& Biases (wandb) is a tool for tracking and visualizing machine learning experiments.
It’s free for open source projects and has a free tier for commercial projects.
It’s a great tool for tracking experiments, visualizing results, and sharing results with collaborators.

\sphinxAtStartPar
If you want to use wandb for hyperparameter sweeps, simply follow these instructions:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Edit \sphinxstyleemphasis{config/sweep/config\_sweep\_ppo.yaml} or create an own sweep config.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Organizing\_Hyperparameter\_Sweeps\_in\_PyTorch\_with\_W\%26B.ipynb}{Here} you can find instructions for creating a configuration.

\item {} 
\sphinxAtStartPar
Make sure that you point to the right training config in your sweep config file.

\item {} 
\sphinxAtStartPar
In the training config which you are using as the basis for the sweep, make sure to track the right success metric. You find it under “benchmarking” along with instructions.

\end{itemize}

\item {} 
\sphinxAtStartPar
Run

\begin{sphinxVerbatim}[commandchars=\\\{\}]
wandb sweep \PYGZhy{}e your\PYGZus{}entity \PYGZhy{}p your\PYGZus{}project config/sweep/config\PYGZus{}sweep\PYGZus{}ppo.yaml to create a wandb sweep.
\end{sphinxVerbatim}

\item {} 
\sphinxAtStartPar
Run

\begin{sphinxVerbatim}[commandchars=\\\{\}]
wandb agent your\PYGZus{}entity/your\PYGZus{}project/sweep\PYGZus{}name to start the sweep
\end{sphinxVerbatim}

\end{enumerate}


\section{API Reference}
\label{\detokenize{api:api-reference}}\label{\detokenize{api::doc}}

\subsection{Reinforcement Learning Agents}
\label{\detokenize{agents.reinforcement_learning:reinforcement-learning-agents}}\label{\detokenize{agents.reinforcement_learning::doc}}

\subsubsection{DQN}
\label{\detokenize{agents.reinforcement_learning:module-agents.reinforcement_learning.dqn}}\label{\detokenize{agents.reinforcement_learning:dqn}}\index{module@\spxentry{module}!agents.reinforcement\_learning.dqn@\spxentry{agents.reinforcement\_learning.dqn}}\index{agents.reinforcement\_learning.dqn@\spxentry{agents.reinforcement\_learning.dqn}!module@\spxentry{module}}
\sphinxAtStartPar
DQN Implementation with target net and epsilon greedy. Follows the Stable Baselines 3 implementation.
To reuse trained models, you can make use of the save and load function.
To adapt policy and value network structure, specify the layer and activation parameter in your train config or
change the constants in this file
\index{MemoryBuffer (class in agents.reinforcement\_learning.dqn)@\spxentry{MemoryBuffer}\spxextra{class in agents.reinforcement\_learning.dqn}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.MemoryBuffer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.dqn.}}\sphinxbfcode{\sphinxupquote{MemoryBuffer}}}{\emph{\DUrole{n}{buffer\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{batch\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{obs\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{obs\_type}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{type}}, \emph{\DUrole{n}{action\_type}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{type}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
Handles episode data collection and sample generation
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{buffer\_size}} \textendash{} Buffer size

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} \textendash{} Size for batches to be generated

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{obs\_dim}} \textendash{} Size of the observation to be stored in the buffer

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{obs\_type}} \textendash{} Type of the observation to be stored in the buffer

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_type}} \textendash{} Type of the action to be stored in the buffer

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (agents.reinforcement\_learning.dqn.MemoryBuffer method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.dqn.MemoryBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.MemoryBuffer.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{buffer\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{batch\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{obs\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{obs\_type}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{type}}, \emph{\DUrole{n}{action\_type}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{type}}}{}
\end{fulllineitems}

\index{store\_memory() (agents.reinforcement\_learning.dqn.MemoryBuffer method)@\spxentry{store\_memory()}\spxextra{agents.reinforcement\_learning.dqn.MemoryBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.MemoryBuffer.store_memory}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{store\_memory}}}{\emph{\DUrole{n}{obs}}, \emph{\DUrole{n}{action}}, \emph{\DUrole{n}{reward}}, \emph{\DUrole{n}{done}}, \emph{\DUrole{n}{new\_obs}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Appends all data from the recent step
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{obs}} \textendash{} Observation at the beginning of the step

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action}} \textendash{} Index of the selected action

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reward}} \textendash{} Reward the env returned in this step

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{done}} \textendash{} True if the episode ended in this step

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{new\_obs}} \textendash{} Observation after the step

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar


\end{description}\end{quote}

\end{fulllineitems}

\index{get\_samples() (agents.reinforcement\_learning.dqn.MemoryBuffer method)@\spxentry{get\_samples()}\spxextra{agents.reinforcement\_learning.dqn.MemoryBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.MemoryBuffer.get_samples}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_samples}}}{}{{ $\rightarrow$ Tuple}}
\sphinxAtStartPar
Generates random samples from the stored data
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
batch\_size samples from the buffer. e.g. obs, actions, …, new\_obs from step 21

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Policy (class in agents.reinforcement\_learning.dqn)@\spxentry{Policy}\spxextra{class in agents.reinforcement\_learning.dqn}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.Policy}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.dqn.}}\sphinxbfcode{\sphinxupquote{Policy}}}{\emph{\DUrole{n}{obs\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{action\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{learning\_rate}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{hidden\_layers}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{activation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

\sphinxAtStartPar
Network structure used for both the Q network and the target network
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{obs\_dim}} \textendash{} Observation size to determine input dimension

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_dim}} \textendash{} Number of action to determine output size

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{learning\_rate}} \textendash{} Learning rate for the network

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_layers}} \textendash{} List of hidden layer sizes (int)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{activation}} \textendash{} String naming activation function for hidden layers

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (agents.reinforcement\_learning.dqn.Policy method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.dqn.Policy method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.Policy.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{obs\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{action\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{learning\_rate}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{hidden\_layers}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{activation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{}
\sphinxAtStartPar
Initializes internal Module state, shared by both nn.Module and ScriptModule.

\end{fulllineitems}

\index{forward() (agents.reinforcement\_learning.dqn.Policy method)@\spxentry{forward()}\spxextra{agents.reinforcement\_learning.dqn.Policy method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.Policy.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{obs}}}{}
\sphinxAtStartPar
forward pass through the Q\sphinxhyphen{}network

\end{fulllineitems}

\index{training (agents.reinforcement\_learning.dqn.Policy attribute)@\spxentry{training}\spxextra{agents.reinforcement\_learning.dqn.Policy attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.Policy.training}}\pysigline{\sphinxbfcode{\sphinxupquote{training}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }bool}}}
\end{fulllineitems}


\end{fulllineitems}

\index{DQN (class in agents.reinforcement\_learning.dqn)@\spxentry{DQN}\spxextra{class in agents.reinforcement\_learning.dqn}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.DQN}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.dqn.}}\sphinxbfcode{\sphinxupquote{DQN}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}src.utils.logger.Logger\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
DQN Implementation with target net and epsilon greedy. Follows the Stable Baselines 3 implementation.
\index{\_\_init\_\_() (agents.reinforcement\_learning.dqn.DQN method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.dqn.DQN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.DQN.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}src.utils.logger.Logger\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\begin{DUlineblock}{0em}
\item[] batch\_size: Number of samples that are chosen and passed through the net per update
\item[] gradient\_steps: Number of updates per training
\item[] train\_freq: Environment steps between two trainings
\item[] buffer\_size: Size of the memory buffer = max number of rollouts that can be stored before the oldest are deleted
\item[] target\_net\_update: Number of steps between target\_net\_updates
\item[] training\_starts = Learning\_starts: steps after which training can start for the first time
\item[] initial\_eps: Initial epsilon value
\item[] final\_eps: Final epsilon value
\item[] fraction\_eps: If the percentage progress of learn exceeds fraction eps, epsilon takes the final\_eps value
\item[] e.g. 5/100 total\_timesteps done \sphinxhyphen{}\textgreater{} progress = 0.5 \textgreater{} fraction eps \sphinxhyphen{}\textgreater{} eps=final\_eps
\item[] max\_grad\_norm: Value to clip the policy update of the q\_net
\end{DUlineblock}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{env}} \textendash{} Pregenerated, gymbased environment. If no env is passed, env = None \sphinxhyphen{}\textgreater{} PPO can only be used
for evaluation (action prediction)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Dictionary with parameters to specify DQN attributes

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logger}} \textendash{} Logger

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{save() (agents.reinforcement\_learning.dqn.DQN method)@\spxentry{save()}\spxextra{agents.reinforcement\_learning.dqn.DQN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.DQN.save}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{save}}}{\emph{\DUrole{n}{file}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Save model as pickle file
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{file}} \textendash{} Path under which the file will be saved

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{load() (agents.reinforcement\_learning.dqn.DQN class method)@\spxentry{load()}\spxextra{agents.reinforcement\_learning.dqn.DQN class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.DQN.load}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{load}}}{\emph{\DUrole{n}{file}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}src.utils.logger.Logger\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Creates a DQN object according to the parameters saved in file.pkl
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{file}} \textendash{} Path and filname (without .pkl) of your saved model pickle file

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Dictionary with parameters to specify PPO attributes

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logger}} \textendash{} Logger

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
DQN object

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_action() (agents.reinforcement\_learning.dqn.DQN method)@\spxentry{get\_action()}\spxextra{agents.reinforcement\_learning.dqn.DQN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.DQN.get_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_action}}}{\emph{\DUrole{n}{obs}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}}{{ $\rightarrow$ int}}
\sphinxAtStartPar
Random action or action according to the policy and epsilon
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
action index

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (agents.reinforcement\_learning.dqn.DQN method)@\spxentry{predict()}\spxextra{agents.reinforcement\_learning.dqn.DQN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.DQN.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{\DUrole{n}{observation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}, \emph{\DUrole{n}{action\_mask}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{array({[}1.0{]})}}, \emph{\DUrole{n}{deterministic}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{True}}, \emph{\DUrole{n}{state}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ Tuple}}
\sphinxAtStartPar
Action prediction for testing
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observation}} \textendash{} Current observation of teh environment

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_mask}} \textendash{} Mask of actions, which can logically be taken. NOTE: currently not implemented!

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{deterministic}} \textendash{} Set True, to force a deterministic prediction

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{state}} \textendash{} The last states (used in rnn policies)

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Predicted action and next state (used in rnn policies)

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (agents.reinforcement\_learning.dqn.DQN method)@\spxentry{train()}\spxextra{agents.reinforcement\_learning.dqn.DQN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.DQN.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Trains Q\sphinxhyphen{}network and Target\sphinxhyphen{}Network
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{on\_step() (agents.reinforcement\_learning.dqn.DQN method)@\spxentry{on\_step()}\spxextra{agents.reinforcement\_learning.dqn.DQN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.DQN.on_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_step}}}{\emph{\DUrole{n}{total\_timesteps}}}{}
\sphinxAtStartPar
Method track and check plenty conditions to e.g. check if q\_target\_net or epsilon update are necessary

\end{fulllineitems}

\index{learn() (agents.reinforcement\_learning.dqn.DQN method)@\spxentry{learn()}\spxextra{agents.reinforcement\_learning.dqn.DQN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.dqn.DQN.learn}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{learn}}}{\emph{\DUrole{n}{total\_instances}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{total\_timesteps}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{intermediate\_test}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Learn over n problem instances or n timesteps (environment steps).
Breaks depending on which condition is met first.
One learning iteration consists of collecting rollouts and training the networks on the rollout data
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{total\_instances}} \textendash{} Instance limit

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{total\_timesteps}} \textendash{} Timestep limit

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{intermediate\_test}} \textendash{} (IntermediateTest) intermediate test object. Must be created before.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{PPO}
\label{\detokenize{agents.reinforcement_learning:module-agents.reinforcement_learning.ppo}}\label{\detokenize{agents.reinforcement_learning:ppo}}\index{module@\spxentry{module}!agents.reinforcement\_learning.ppo@\spxentry{agents.reinforcement\_learning.ppo}}\index{agents.reinforcement\_learning.ppo@\spxentry{agents.reinforcement\_learning.ppo}!module@\spxentry{module}}
\sphinxAtStartPar
PPO implementation inspired by the StableBaselines3 implementation.
To reuse trained models, you can make use of the save and load function
To adapt policy and value network structure, specify the policy and value layer and activation parameter
in your train config or change the constants in this file
\index{RolloutBuffer (class in agents.reinforcement\_learning.ppo)@\spxentry{RolloutBuffer}\spxextra{class in agents.reinforcement\_learning.ppo}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.RolloutBuffer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.ppo.}}\sphinxbfcode{\sphinxupquote{RolloutBuffer}}}{\emph{\DUrole{n}{buffer\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{batch\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
Handles episode data collection and batch generation
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{buffer\_size}} \textendash{} Buffer size

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} \textendash{} Size for batches to be generated

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (agents.reinforcement\_learning.ppo.RolloutBuffer method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.ppo.RolloutBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.RolloutBuffer.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{buffer\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{batch\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}}{}
\end{fulllineitems}

\index{generate\_batches() (agents.reinforcement\_learning.ppo.RolloutBuffer method)@\spxentry{generate\_batches()}\spxextra{agents.reinforcement\_learning.ppo.RolloutBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.RolloutBuffer.generate_batches}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_batches}}}{}{{ $\rightarrow$ Tuple}}
\sphinxAtStartPar
Generates batches from the stored data
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
batches: Lists with all indices from the rollout\_data, shuffled and sampled in lists with batch\_size
e.g. {[}{[}0,34,1,768,…(len: batch size){]}, {[}{]}, …(len: len(rollout\_data) / batch size){]}

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_advantages\_and\_returns() (agents.reinforcement\_learning.ppo.RolloutBuffer method)@\spxentry{compute\_advantages\_and\_returns()}\spxextra{agents.reinforcement\_learning.ppo.RolloutBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.RolloutBuffer.compute_advantages_and_returns}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{compute\_advantages\_and\_returns}}}{\emph{\DUrole{n}{last\_value}}, \emph{\DUrole{n}{gamma}}, \emph{\DUrole{n}{gae\_lambda}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Computes advantage values and returns for all stored episodes.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{last\_value}} \textendash{} Value from the next step to calculate the advantage for the last episode in the buffer

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gamma}} \textendash{} Discount factor for the advantage calculation

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gae\_lambda}} \textendash{} Smoothing parameter for the advantage calculation

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{store\_memory() (agents.reinforcement\_learning.ppo.RolloutBuffer method)@\spxentry{store\_memory()}\spxextra{agents.reinforcement\_learning.ppo.RolloutBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.RolloutBuffer.store_memory}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{store\_memory}}}{\emph{\DUrole{n}{observation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}, \emph{\DUrole{n}{action}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{prob}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{value}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{reward}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Any}}, \emph{\DUrole{n}{done}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Appends all data from the recent step
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observation}} \textendash{} Observation at the beginning of the step

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action}} \textendash{} Index of the selected action

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prob}} \textendash{} Probability of the selected action (output from the policy\_net)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{value}} \textendash{} Baseline value that the value\_net estimated from this step onwards according to the

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observation}} \textendash{} Output from the value\_net

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reward}} \textendash{} Reward the env returned in this step

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{done}} \textendash{} True if the episode ended in this step

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (agents.reinforcement\_learning.ppo.RolloutBuffer method)@\spxentry{reset()}\spxextra{agents.reinforcement\_learning.ppo.RolloutBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.RolloutBuffer.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Resets all buffer lists
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{PolicyNetwork (class in agents.reinforcement\_learning.ppo)@\spxentry{PolicyNetwork}\spxextra{class in agents.reinforcement\_learning.ppo}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PolicyNetwork}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.ppo.}}\sphinxbfcode{\sphinxupquote{PolicyNetwork}}}{\emph{\DUrole{n}{input\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{n\_actions}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{learning\_rate}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{hidden\_layers}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{activation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

\sphinxAtStartPar
Policy Network for the agent
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{input\_dim}} \textendash{} Observation size to determine input dimension

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{n\_actions}} \textendash{} Number of action to determine output size

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{learning\_rate}} \textendash{} Learning rate for the network

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_layers}} \textendash{} List of hidden layer sizes (int)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{activation}} \textendash{} String naming activation function for hidden layers

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (agents.reinforcement\_learning.ppo.PolicyNetwork method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.ppo.PolicyNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PolicyNetwork.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{input\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{n\_actions}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{learning\_rate}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{hidden\_layers}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{activation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{}
\sphinxAtStartPar
Initializes internal Module state, shared by both nn.Module and ScriptModule.

\end{fulllineitems}

\index{forward() (agents.reinforcement\_learning.ppo.PolicyNetwork method)@\spxentry{forward()}\spxextra{agents.reinforcement\_learning.ppo.PolicyNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PolicyNetwork.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{observation}}}{}
\sphinxAtStartPar
forward function

\end{fulllineitems}

\index{training (agents.reinforcement\_learning.ppo.PolicyNetwork attribute)@\spxentry{training}\spxextra{agents.reinforcement\_learning.ppo.PolicyNetwork attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PolicyNetwork.training}}\pysigline{\sphinxbfcode{\sphinxupquote{training}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }bool}}}
\end{fulllineitems}


\end{fulllineitems}

\index{ValueNetwork (class in agents.reinforcement\_learning.ppo)@\spxentry{ValueNetwork}\spxextra{class in agents.reinforcement\_learning.ppo}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.ValueNetwork}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.ppo.}}\sphinxbfcode{\sphinxupquote{ValueNetwork}}}{\emph{\DUrole{n}{input\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{learning\_rate}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{hidden\_layers}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{activation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

\sphinxAtStartPar
Value Network for the agent
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{input\_dim}} \textendash{} Observation size to determine input dimension

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{learning\_rate}} \textendash{} Learning rate for the network

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{hidden\_layers}} \textendash{} List of hidden layer sizes (int)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{activation}} \textendash{} String naming activation function for hidden layers

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (agents.reinforcement\_learning.ppo.ValueNetwork method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.ppo.ValueNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.ValueNetwork.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{input\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{learning\_rate}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{hidden\_layers}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{activation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{}
\sphinxAtStartPar
Initializes internal Module state, shared by both nn.Module and ScriptModule.

\end{fulllineitems}

\index{forward() (agents.reinforcement\_learning.ppo.ValueNetwork method)@\spxentry{forward()}\spxextra{agents.reinforcement\_learning.ppo.ValueNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.ValueNetwork.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{observation}}}{}
\sphinxAtStartPar
forward function

\end{fulllineitems}

\index{training (agents.reinforcement\_learning.ppo.ValueNetwork attribute)@\spxentry{training}\spxextra{agents.reinforcement\_learning.ppo.ValueNetwork attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.ValueNetwork.training}}\pysigline{\sphinxbfcode{\sphinxupquote{training}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }bool}}}
\end{fulllineitems}


\end{fulllineitems}

\index{PPO (class in agents.reinforcement\_learning.ppo)@\spxentry{PPO}\spxextra{class in agents.reinforcement\_learning.ppo}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PPO}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.ppo.}}\sphinxbfcode{\sphinxupquote{PPO}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}src.utils.logger.Logger\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
PPO Agent class
\index{\_\_init\_\_() (agents.reinforcement\_learning.ppo.PPO method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.ppo.PPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PPO.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}src.utils.logger.Logger\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\begin{DUlineblock}{0em}
\item[] gamma: Discount factor for the advantage calculation
\item[] learning\_rate: Learning rate for both, policy\_net and value\_net
\item[] gae\_lambda: Smoothing parameter for the advantage calculation
\item[] clip\_range: Limitation for the ratio between old and new policy
\item[] batch\_size: Size of batches which were sampled from the buffer and fed into the nets during training
\item[] n\_epochs: Number of repetitions for each training iteration
\item[] rollout\_steps: Step interval within the update is performed. Has to be a multiple of batch\_size
\end{DUlineblock}

\end{fulllineitems}

\index{load() (agents.reinforcement\_learning.ppo.PPO class method)@\spxentry{load()}\spxextra{agents.reinforcement\_learning.ppo.PPO class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PPO.load}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{load}}}{\emph{\DUrole{n}{file}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}src.utils.logger.Logger\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Creates a PPO object according to the parameters saved in file.pkl
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{file}} \textendash{} Path and filname (without .pkl) of your saved model pickle file

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Dictionary with parameters to specify PPO attributes

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logger}} \textendash{} Logger

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
MaskedPPO object

\end{description}\end{quote}

\end{fulllineitems}

\index{save() (agents.reinforcement\_learning.ppo.PPO method)@\spxentry{save()}\spxextra{agents.reinforcement\_learning.ppo.PPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PPO.save}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{save}}}{\emph{\DUrole{n}{file}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Save model as pickle file
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{file}} \textendash{} Path under which the file will be saved

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (agents.reinforcement\_learning.ppo.PPO method)@\spxentry{forward()}\spxextra{agents.reinforcement\_learning.ppo.PPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PPO.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{observation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ Tuple}}
\sphinxAtStartPar
Predicts an action according to the current policy based on the observation
and the value for the next state
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observation}} \textendash{} Current observation of teh environment

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kwargs}} \textendash{} Used to accept but ignore passing actions masks from the environment.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Predicted action, probability for this action, and predicted value for the next state

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (agents.reinforcement\_learning.ppo.PPO method)@\spxentry{predict()}\spxextra{agents.reinforcement\_learning.ppo.PPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PPO.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{\DUrole{n}{observation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}, \emph{\DUrole{n}{deterministic}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{True}}, \emph{\DUrole{n}{state}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ Tuple}}\begin{quote}

\sphinxAtStartPar
Action prediction for testing
\end{quote}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observation}} \textendash{} Current observation of teh environment

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{deterministic}} \textendash{} Set True, to force a deterministic prediction

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{state}} \textendash{} The last states (used in rnn policies)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kwargs}} \textendash{} Used to accept but ignore passing actions masks from the environment.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Predicted action and next state (used in rnn policies)

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (agents.reinforcement\_learning.ppo.PPO method)@\spxentry{train()}\spxextra{agents.reinforcement\_learning.ppo.PPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PPO.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Trains policy and value
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{learn() (agents.reinforcement\_learning.ppo.PPO method)@\spxentry{learn()}\spxextra{agents.reinforcement\_learning.ppo.PPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.PPO.learn}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{learn}}}{\emph{\DUrole{n}{total\_instances}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{total\_timesteps}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{intermediate\_test}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Learn over n environment instances or n timesteps. Break depending on which condition is met first
One learning iteration consists of collecting rollouts and training the networks
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{total\_instances}} \textendash{} Instance limit

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{total\_timesteps}} \textendash{} Timestep limit

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{intermediate\_test}} \textendash{} (IntermediateTest) intermediate test object. Must be created before.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{explained\_variance() (in module agents.reinforcement\_learning.ppo)@\spxentry{explained\_variance()}\spxextra{in module agents.reinforcement\_learning.ppo}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo.explained_variance}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.ppo.}}\sphinxbfcode{\sphinxupquote{explained\_variance}}}{\emph{\DUrole{n}{y\_pred}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}, \emph{\DUrole{n}{y\_true}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}}{{ $\rightarrow$ numpy.ndarray}}
\sphinxAtStartPar
From Stable\sphinxhyphen{}Baseline
Computes fraction of variance that ypred explains about y.
Returns 1 \sphinxhyphen{} Var{[}y\sphinxhyphen{}ypred{]} / Var{[}y{]}
\begin{description}
\item[{interpretation:}] \leavevmode
\sphinxAtStartPar
ev=0  =\textgreater{}  might as well have predicted zero
ev=1  =\textgreater{}  perfect prediction
ev\textless{}0  =\textgreater{}  worse than just predicting zero

\end{description}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y\_pred}} \textendash{} the prediction

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y\_true}} \textendash{} the expected value

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
explained variance of ypred and y

\end{description}\end{quote}

\end{fulllineitems}



\subsubsection{PPO\_masked}
\label{\detokenize{agents.reinforcement_learning:module-agents.reinforcement_learning.ppo_masked}}\label{\detokenize{agents.reinforcement_learning:ppo-masked}}\index{module@\spxentry{module}!agents.reinforcement\_learning.ppo\_masked@\spxentry{agents.reinforcement\_learning.ppo\_masked}}\index{agents.reinforcement\_learning.ppo\_masked@\spxentry{agents.reinforcement\_learning.ppo\_masked}!module@\spxentry{module}}
\sphinxAtStartPar
PPO implementation with action mask according to the StableBaselines3 implementation.
To reuse trained models, you can make use of the save and load function
\index{RolloutBuffer (class in agents.reinforcement\_learning.ppo\_masked)@\spxentry{RolloutBuffer}\spxextra{class in agents.reinforcement\_learning.ppo\_masked}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.RolloutBuffer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.ppo\_masked.}}\sphinxbfcode{\sphinxupquote{RolloutBuffer}}}{\emph{\DUrole{n}{buffer\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{batch\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
Handles episode data collection and batch generation
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{buffer\_size}} \textendash{} Buffer size

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{batch\_size}} \textendash{} Size for batches to be generated

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (agents.reinforcement\_learning.ppo\_masked.RolloutBuffer method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.ppo\_masked.RolloutBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.RolloutBuffer.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{buffer\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{batch\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}}{}
\end{fulllineitems}

\index{generate\_batches() (agents.reinforcement\_learning.ppo\_masked.RolloutBuffer method)@\spxentry{generate\_batches()}\spxextra{agents.reinforcement\_learning.ppo\_masked.RolloutBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.RolloutBuffer.generate_batches}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{generate\_batches}}}{}{{ $\rightarrow$ Tuple}}
\sphinxAtStartPar
Generates batches from the stored data
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
batches: Lists with all indices from the rollout\_data, shuffled and sampled in lists with batch\_size
e.g. {[}{[}0,34,1,768,…(len: batch size){]}, {[}{]}, …(len: len(rollout\_data) / batch size){]}

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_advantages\_and\_returns() (agents.reinforcement\_learning.ppo\_masked.RolloutBuffer method)@\spxentry{compute\_advantages\_and\_returns()}\spxextra{agents.reinforcement\_learning.ppo\_masked.RolloutBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.RolloutBuffer.compute_advantages_and_returns}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{compute\_advantages\_and\_returns}}}{\emph{\DUrole{n}{last\_value}}, \emph{\DUrole{n}{gamma}}, \emph{\DUrole{n}{gae\_lambda}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Computes advantage values and returns for all stored episodes. Required to
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{last\_value}} \textendash{} Value from the next step to calculate the advantage for the last episode in the buffer

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gamma}} \textendash{} Discount factor for the advantage calculation

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gae\_lambda}} \textendash{} Smoothing parameter for the advantage calculation

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{store\_memory() (agents.reinforcement\_learning.ppo\_masked.RolloutBuffer method)@\spxentry{store\_memory()}\spxextra{agents.reinforcement\_learning.ppo\_masked.RolloutBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.RolloutBuffer.store_memory}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{store\_memory}}}{\emph{\DUrole{n}{observation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}, \emph{\DUrole{n}{action}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{prob}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{value}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{reward}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Any}}, \emph{\DUrole{n}{done}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}}, \emph{\DUrole{n}{action\_mask}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Appends all data from the recent step
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observation}} \textendash{} Observation at the beginning of the step

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action}} \textendash{} Index of the selected action

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prob}} \textendash{} Probability of the selected action (output from the policy\_net)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{value}} \textendash{} Baseline value that the value\_net estimated from this step onwards according to the

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observation}} \textendash{} Output from the value\_net

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reward}} \textendash{} Reward the env returned in this step

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{done}} \textendash{} True if the episode ended in this step

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_mask}} \textendash{} One hot vector with ones for all possible actions

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (agents.reinforcement\_learning.ppo\_masked.RolloutBuffer method)@\spxentry{reset()}\spxextra{agents.reinforcement\_learning.ppo\_masked.RolloutBuffer method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.RolloutBuffer.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Resets all buffer lists
:return: None

\end{fulllineitems}


\end{fulllineitems}

\index{PolicyNetwork (class in agents.reinforcement\_learning.ppo\_masked)@\spxentry{PolicyNetwork}\spxextra{class in agents.reinforcement\_learning.ppo\_masked}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.PolicyNetwork}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.ppo\_masked.}}\sphinxbfcode{\sphinxupquote{PolicyNetwork}}}{\emph{\DUrole{n}{input\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{n\_actions}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{learning\_rate}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{hidden\_layers}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{activation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

\sphinxAtStartPar
Policy Network for the agent
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{input\_dims}} \textendash{} Observation size to determine input dimension

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{n\_actions}} \textendash{} Number of action to determine output size

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{learning\_rate}} \textendash{} Learning rate for the network

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{fc1\_dims}} \textendash{} Size hidden layer 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{fc2\_dims}} \textendash{} Size hidden layer 2

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (agents.reinforcement\_learning.ppo\_masked.PolicyNetwork method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.ppo\_masked.PolicyNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.PolicyNetwork.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{input\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{n\_actions}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{learning\_rate}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{hidden\_layers}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{activation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{}
\sphinxAtStartPar
Initializes internal Module state, shared by both nn.Module and ScriptModule.

\end{fulllineitems}

\index{forward() (agents.reinforcement\_learning.ppo\_masked.PolicyNetwork method)@\spxentry{forward()}\spxextra{agents.reinforcement\_learning.ppo\_masked.PolicyNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.PolicyNetwork.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{observation}}, \emph{\DUrole{n}{action\_mask}}}{}
\sphinxAtStartPar
forward through the actor network

\end{fulllineitems}

\index{training (agents.reinforcement\_learning.ppo\_masked.PolicyNetwork attribute)@\spxentry{training}\spxextra{agents.reinforcement\_learning.ppo\_masked.PolicyNetwork attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.PolicyNetwork.training}}\pysigline{\sphinxbfcode{\sphinxupquote{training}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }bool}}}
\end{fulllineitems}


\end{fulllineitems}

\index{ValueNetwork (class in agents.reinforcement\_learning.ppo\_masked)@\spxentry{ValueNetwork}\spxextra{class in agents.reinforcement\_learning.ppo\_masked}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.ValueNetwork}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.ppo\_masked.}}\sphinxbfcode{\sphinxupquote{ValueNetwork}}}{\emph{\DUrole{n}{input\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{learning\_rate}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{hidden\_layers}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{activation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

\sphinxAtStartPar
Value Network for the agent
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{input\_dims}} \textendash{} Observation size to determine input dimension

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{learning\_rate}} \textendash{} Learning rate for the network

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{fc1\_dims}} \textendash{} Size hidden layer 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{fc2\_dims}} \textendash{} Size hidden layer 2

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (agents.reinforcement\_learning.ppo\_masked.ValueNetwork method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.ppo\_masked.ValueNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.ValueNetwork.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{input\_dim}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{learning\_rate}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{float}}, \emph{\DUrole{n}{hidden\_layers}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{activation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{}
\sphinxAtStartPar
Initializes internal Module state, shared by both nn.Module and ScriptModule.

\end{fulllineitems}

\index{forward() (agents.reinforcement\_learning.ppo\_masked.ValueNetwork method)@\spxentry{forward()}\spxextra{agents.reinforcement\_learning.ppo\_masked.ValueNetwork method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.ValueNetwork.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{observation}}}{}
\sphinxAtStartPar
forward through the value network

\end{fulllineitems}

\index{training (agents.reinforcement\_learning.ppo\_masked.ValueNetwork attribute)@\spxentry{training}\spxextra{agents.reinforcement\_learning.ppo\_masked.ValueNetwork attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.ValueNetwork.training}}\pysigline{\sphinxbfcode{\sphinxupquote{training}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }bool}}}
\end{fulllineitems}


\end{fulllineitems}

\index{MaskedPPO (class in agents.reinforcement\_learning.ppo\_masked)@\spxentry{MaskedPPO}\spxextra{class in agents.reinforcement\_learning.ppo\_masked}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.MaskedPPO}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.ppo\_masked.}}\sphinxbfcode{\sphinxupquote{MaskedPPO}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}src.utils.logger.Logger\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}
\index{\_\_init\_\_() (agents.reinforcement\_learning.ppo\_masked.MaskedPPO method)@\spxentry{\_\_init\_\_()}\spxextra{agents.reinforcement\_learning.ppo\_masked.MaskedPPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.MaskedPPO.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}src.utils.logger.Logger\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\begin{DUlineblock}{0em}
\item[] gamma: Discount factor for the advantage calculation
\item[] learning\_rate: Learning rate for both, policy\_net and value\_net
\item[] gae\_lambda: Smoothing parameter for the advantage calculation
\item[] clip\_range: Limitation for the ratio between old and new policy
\item[] batch\_size: Size of batches which were sampled from the buffer and fed into the nets during training
\item[] n\_epochs: Number of repetitions for each training iteration
\item[] rollout\_steps: Step interval within the update is performed. Has to be a multiple of batch\_size
\end{DUlineblock}

\end{fulllineitems}

\index{load() (agents.reinforcement\_learning.ppo\_masked.MaskedPPO class method)@\spxentry{load()}\spxextra{agents.reinforcement\_learning.ppo\_masked.MaskedPPO class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.MaskedPPO.load}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{load}}}{\emph{\DUrole{n}{file}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}src.utils.logger.Logger\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Creates a PPO object according to the parameters saved in file.pkl
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{file}} \textendash{} Path and filname (without .pkl) of your saved model pickle file

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Dictionary with parameters to specify PPO attributes

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logger}} \textendash{} Logger

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
MaskedPPO object

\end{description}\end{quote}

\end{fulllineitems}

\index{save() (agents.reinforcement\_learning.ppo\_masked.MaskedPPO method)@\spxentry{save()}\spxextra{agents.reinforcement\_learning.ppo\_masked.MaskedPPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.MaskedPPO.save}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{save}}}{\emph{\DUrole{n}{file}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Save model as pickle file
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{file}} \textendash{} Path under which the file will be saved

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (agents.reinforcement\_learning.ppo\_masked.MaskedPPO method)@\spxentry{forward()}\spxextra{agents.reinforcement\_learning.ppo\_masked.MaskedPPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.MaskedPPO.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{observation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}, \emph{\DUrole{n}{action\_mask}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}}{{ $\rightarrow$ Tuple}}
\sphinxAtStartPar
Predicts an action according to the current policy and based on the action\_mask and observation
and the value for the next state
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observation}} \textendash{} Current observation of teh environment

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_mask}} \textendash{} One hot vector with ones for all possible actions

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Predicted action, probability for this action, and predicted value for the next state

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (agents.reinforcement\_learning.ppo\_masked.MaskedPPO method)@\spxentry{predict()}\spxextra{agents.reinforcement\_learning.ppo\_masked.MaskedPPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.MaskedPPO.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{\DUrole{n}{observation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}, \emph{\DUrole{n}{action\_mask}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}, \emph{\DUrole{n}{deterministic}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{True}}, \emph{\DUrole{n}{state}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ Tuple}}
\sphinxAtStartPar
Action prediction for testing
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observation}} \textendash{} Current observation of teh environment

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_mask}} \textendash{} One hot vector with ones for all possible actions

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{deterministic}} \textendash{} Set True, to force a deterministic prediction

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{state}} \textendash{} The last states (used in rnn policies)

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Predicted action and next state (used in rnn policies)

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (agents.reinforcement\_learning.ppo\_masked.MaskedPPO method)@\spxentry{train()}\spxextra{agents.reinforcement\_learning.ppo\_masked.MaskedPPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.MaskedPPO.train}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Trains policy and value
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{learn() (agents.reinforcement\_learning.ppo\_masked.MaskedPPO method)@\spxentry{learn()}\spxextra{agents.reinforcement\_learning.ppo\_masked.MaskedPPO method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.MaskedPPO.learn}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{learn}}}{\emph{\DUrole{n}{total\_instances}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{total\_timesteps}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{intermediate\_test}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Learn over n environment instances or n timesteps. Break depending on which condition is met first
One learning iteration consists of collecting rollouts and training the networks
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{total\_instances}} \textendash{} Instance limit

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{total\_timesteps}} \textendash{} Timestep limit

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{intermediate\_test}} \textendash{} (IntermediateTest) intermediate test object. Must be created before.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{explained\_variance() (in module agents.reinforcement\_learning.ppo\_masked)@\spxentry{explained\_variance()}\spxextra{in module agents.reinforcement\_learning.ppo\_masked}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.reinforcement_learning.ppo_masked.explained_variance}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.reinforcement\_learning.ppo\_masked.}}\sphinxbfcode{\sphinxupquote{explained\_variance}}}{\emph{\DUrole{n}{y\_pred}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}, \emph{\DUrole{n}{y\_true}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.ndarray}}}{{ $\rightarrow$ numpy.ndarray}}
\sphinxAtStartPar
From Stable\sphinxhyphen{}Baseline
Computes fraction of variance that ypred explains about y.
Returns 1 \sphinxhyphen{} Var{[}y\sphinxhyphen{}ypred{]} / Var{[}y{]}
\begin{description}
\item[{interpretation:}] \leavevmode
\sphinxAtStartPar
ev=0  =\textgreater{}  might as well have predicted zero
ev=1  =\textgreater{}  perfect prediction
ev\textless{}0  =\textgreater{}  worse than just predicting zero

\end{description}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y\_pred}} \textendash{} the prediction

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y\_true}} \textendash{} the expected value

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
explained variance of ypred and y

\end{description}\end{quote}

\end{fulllineitems}



\subsection{Reinforcement Learning Functions}
\label{\detokenize{agents.reinforcement_learning:reinforcement-learning-functions}}

\subsubsection{Training Functions}
\label{\detokenize{agents.reinforcement_learning:module-agents.train}}\label{\detokenize{agents.reinforcement_learning:training-functions}}\index{module@\spxentry{module}!agents.train@\spxentry{agents.train}}\index{agents.train@\spxentry{agents.train}!module@\spxentry{module}}
\sphinxAtStartPar
This file provides functions to train an agent on a scheduling\sphinxhyphen{}problem environment.
By default, the trained model will be evaluated on the test data after training,
by running the test\_model\_and\_heuristic function from test.py.

\sphinxAtStartPar
Using this file requires a training config. For example, you have to specify the algorithm used for the training.

\sphinxAtStartPar
There are several constants, which you can change to adapt the training process:
\index{final\_evaluation() (in module agents.train)@\spxentry{final\_evaluation()}\spxextra{in module agents.train}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.train.final_evaluation}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.train.}}\sphinxbfcode{\sphinxupquote{final\_evaluation}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{data\_test}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.utils.logger.Logger}}}{}
\sphinxAtStartPar
Evaluates the trained model and logs the results
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Training config

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data\_test}} \textendash{} Dataset with instances to be used for the test

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logger}} \textendash{} Logger object

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{training() (in module agents.train)@\spxentry{training()}\spxextra{in module agents.train}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.train.training}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.train.}}\sphinxbfcode{\sphinxupquote{training}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{data\_train}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{data\_val}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.utils.logger.Logger}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Handles the actual training process.
Including creating the environment, agent and intermediate\_test object. Then the agent learning process is started
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Training config

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data\_train}} \textendash{} Dataset with instances to be used for the training

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data\_val}} \textendash{} Dataset with instances to be used for the evaluation

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logger}} \textendash{} Logger object used for the whole training process, including evaluation and testing

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{main() (in module agents.train)@\spxentry{main()}\spxextra{in module agents.train}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.train.main}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.train.}}\sphinxbfcode{\sphinxupquote{main}}}{\emph{\DUrole{n}{config\_file\_name}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}dict\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{external\_config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}dict\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Main function to train an agent in a scheduling\sphinxhyphen{}problem environment.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config\_file\_name}} \textendash{} path to the training config you want to use for training
(relative path from config/ folder)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{external\_config}} \textendash{} dictionary that can be passed to overwrite the config file elements

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_perser\_args() (in module agents.train)@\spxentry{get\_perser\_args()}\spxextra{in module agents.train}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.train.get_perser_args}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.train.}}\sphinxbfcode{\sphinxupquote{get\_perser\_args}}}{}{}
\sphinxAtStartPar
Get arguments from command line

\end{fulllineitems}



\subsubsection{Testing Functions}
\label{\detokenize{agents.reinforcement_learning:module-agents.test}}\label{\detokenize{agents.reinforcement_learning:testing-functions}}\index{module@\spxentry{module}!agents.test@\spxentry{agents.test}}\index{agents.test@\spxentry{agents.test}!module@\spxentry{module}}
\sphinxAtStartPar
This file provides the test\_model function to evaluate an agent or a heuristic on a set of instances.
Furthermore, test\_model\_and\_heuristics can be used to evaluate an agent and all heuristics specified in the
TEST\_HEURISTICS constant on the same set of instances.

\sphinxAtStartPar
Using this file requires a testing config. For example, it is necessary to specify the name of the model
you want to test.

\sphinxAtStartPar
Running this file will automatically call test\_model\_and\_heuristics.
You can adapt the heuristics used for testing in the TEST\_HEURISTICS constant. An empty list is admissible.

\sphinxAtStartPar
When running the file from a console you can use \textendash{}plot\sphinxhyphen{}ganttchart to show the generated gantt\_chart figures.
\index{get\_action() (in module agents.test)@\spxentry{get\_action()}\spxextra{in module agents.test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.test.get_action}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.test.}}\sphinxbfcode{\sphinxupquote{get\_action}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{n}{model}}, \emph{\DUrole{n}{heuristic\_id}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}, \emph{\DUrole{n}{heuristic\_agent}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}src.agents.heuristic.heuristic\_agent.HeuristicSelectionAgent\DUrole{p}{{]}}}}}{{ $\rightarrow$ Tuple\DUrole{p}{{[}}int\DUrole{p}{,}\DUrole{w}{  }str\DUrole{p}{{]}}}}
\sphinxAtStartPar
This function determines the next action according to the input model or heuristic
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{env}} \textendash{} Environment object

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{model}} \textendash{} Model object. E.g. PPO object

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{heuristic\_id}} \textendash{} Heuristic identifier. Can be None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{heuristic\_agent}} \textendash{} HeuristicSelectionAgent object. Can be None

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
ID of the selected action

\end{description}\end{quote}

\end{fulllineitems}

\index{run\_episode() (in module agents.test)@\spxentry{run\_episode()}\spxextra{in module agents.test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.test.run_episode}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.test.}}\sphinxbfcode{\sphinxupquote{run\_episode}}}{\emph{\DUrole{n}{env}}, \emph{\DUrole{n}{model}}, \emph{\DUrole{n}{heuristic\_id}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}str\DUrole{p}{{]}}}}, \emph{\DUrole{n}{handler}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.utils.evaluations.EvaluationHandler}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
This function executes one testing episode
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{env}} \textendash{} Environment object

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{model}} \textendash{} Model object. E.g. PPO object

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{heuristic\_id}} \textendash{} Heuristic identifier. Can be None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{handler}} \textendash{} EvaluationHandler object

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{test\_solver() (in module agents.test)@\spxentry{test\_solver()}\spxextra{in module agents.test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.test.test_solver}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.test.}}\sphinxbfcode{\sphinxupquote{test\_solver}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Dict}}, \emph{\DUrole{n}{data\_test}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.utils.logger.Logger}}}{{ $\rightarrow$ Dict}}
\sphinxAtStartPar
This function uses the OR solver to schedule the instances given in data\_test.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Testing config

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data\_test}} \textendash{} Data containing problem instances used for testing

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Evaluation metrics

\end{description}\end{quote}

\end{fulllineitems}

\index{log\_results() (in module agents.test)@\spxentry{log\_results()}\spxextra{in module agents.test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.test.log_results}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.test.}}\sphinxbfcode{\sphinxupquote{log\_results}}}{\emph{\DUrole{n}{plot\_logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.utils.logger.Logger}}, \emph{\DUrole{n}{inter\_test\_idx}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}}, \emph{\DUrole{n}{heuristic}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}}, \emph{\DUrole{n}{env}}, \emph{\DUrole{n}{handler}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.utils.evaluations.EvaluationHandler}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Calls the logger object to save the test results from this episode as table (e.g. makespan mean, gantt chart)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{plot\_logger}} \textendash{} Logger object

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{inter\_test\_idx}} \textendash{} Index of current test. Can be None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{heuristic}} \textendash{} Heuristic identifier. Can be None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{env}} \textendash{} Environment object

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{handler}} \textendash{} EvaluationHandler object

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{test\_model() (in module agents.test)@\spxentry{test\_model()}\spxextra{in module agents.test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.test.test_model}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.test.}}\sphinxbfcode{\sphinxupquote{test\_model}}}{\emph{\DUrole{n}{env\_config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Dict}}, \emph{\DUrole{n}{data}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.utils.logger.Logger}}, \emph{\DUrole{n}{plot}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}bool\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{log\_episode}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}bool\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{model}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{heuristic\_id}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}str\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{intermediate\_test\_idx}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ dict}}
\sphinxAtStartPar
This function tests a model in the passed environment for all problem instances passed as data\_test and returns an
evaluation summary
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{env\_config}} \textendash{} Environment config

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data}} \textendash{} Data containing problem instances used for testing

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logger}} \textendash{} Logger object

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{plot}} \textendash{} Plot a gantt chart of all tests

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{log\_episode}} \textendash{} If true, calls the log function to log episode results as table

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{model}} \textendash{} \{None, StableBaselines Model\}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{heuristic\_id}} \textendash{} ID that identifies the used heuristic

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{intermediate\_test\_idx}} \textendash{} Step number after which the test is performed. Is used to annotate the log

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
evaluation metrics

\end{description}\end{quote}

\end{fulllineitems}

\index{test\_model\_and\_heuristic() (in module agents.test)@\spxentry{test\_model\_and\_heuristic()}\spxextra{in module agents.test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.test.test_model_and_heuristic}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.test.}}\sphinxbfcode{\sphinxupquote{test\_model\_and\_heuristic}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{model}}, \emph{\DUrole{n}{data\_test}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.utils.logger.Logger}}, \emph{\DUrole{n}{plot\_ganttchart}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}, \emph{\DUrole{n}{log\_episode}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ dict}}
\sphinxAtStartPar
Test model and agent\_heuristics len(data) times and returns results
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Testing config

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{model}} \textendash{} Model to be tested. E.g. PPO object

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data\_test}} \textendash{} Dataset with instances to be used for the test

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logger}} \textendash{} Logger object

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{plot\_ganttchart}} \textendash{} Plot a gantt chart of all tests

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{log\_episode}} \textendash{} If true, calls the log function to log episode results as table

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Dict with evaluation\_result dicts for the agent and all heuristics which were tested

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_perser\_args() (in module agents.test)@\spxentry{get\_perser\_args()}\spxextra{in module agents.test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.test.get_perser_args}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.test.}}\sphinxbfcode{\sphinxupquote{get\_perser\_args}}}{}{}
\end{fulllineitems}

\index{main() (in module agents.test)@\spxentry{main()}\spxextra{in module agents.test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.test.main}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.test.}}\sphinxbfcode{\sphinxupquote{main}}}{\emph{\DUrole{n}{external\_config}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\end{fulllineitems}



\subsubsection{Util functions}
\label{\detokenize{agents.reinforcement_learning:module-agents.intermediate_test}}\label{\detokenize{agents.reinforcement_learning:util-functions}}\index{module@\spxentry{module}!agents.intermediate\_test@\spxentry{agents.intermediate\_test}}\index{agents.intermediate\_test@\spxentry{agents.intermediate\_test}!module@\spxentry{module}}
\sphinxAtStartPar
This file provides the IntermediateTest class which is used to run an intermediate test on the current model policy.
If the recent model has the best result it is saved as the new current optimum
\index{IntermediateTest (class in agents.intermediate\_test)@\spxentry{IntermediateTest}\spxextra{class in agents.intermediate\_test}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.intermediate_test.IntermediateTest}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.intermediate\_test.}}\sphinxbfcode{\sphinxupquote{IntermediateTest}}}{\emph{\DUrole{n}{env\_config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{n\_test\_steps}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{data}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.utils.logger.Logger}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
This object is used to run an intermediate test on the current model policy.
If the recent model has the best result it is saved as the new current optimum
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{env\_config}} \textendash{} Config used to initialize the environment for training

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{n\_test\_steps}} \textendash{} Number of environment steps between intermediate tests

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data}} \textendash{} Dataset with instances to be used for the intermediate test

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logger}} \textendash{} Logger object

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (agents.intermediate\_test.IntermediateTest method)@\spxentry{\_\_init\_\_()}\spxextra{agents.intermediate\_test.IntermediateTest method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.intermediate_test.IntermediateTest.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{env\_config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{n\_test\_steps}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{data}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{logger}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.utils.logger.Logger}}}{}
\end{fulllineitems}

\index{on\_step() (agents.intermediate\_test.IntermediateTest method)@\spxentry{on\_step()}\spxextra{agents.intermediate\_test.IntermediateTest method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.reinforcement_learning:agents.intermediate_test.IntermediateTest.on_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{on\_step}}}{\emph{\DUrole{n}{num\_timesteps}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{instances}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{model}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
This function is called by the environment during each step.
According to n\_test\_steps the function runs an intermediate test
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_timesteps}} \textendash{} Number of steps that have been already run by the environment

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{instances}} \textendash{} Number of instances that have been already run by the environment

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{model}} \textendash{} Model with the current policy. E.g. PPO object

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}


\sphinxAtStartPar
This file provides utility functions to load configs, data and agents according to the config. It is used in training
and testing.

\sphinxAtStartPar
TIMESTAMP: str: timestamp of the training run, used for the creation of a unique model name
AGENT\_DICT: dict{[}str, str{]}: This dictionary is used to map algorithm identifiers (keys)
to their actual class names (values).

\sphinxAtStartPar
E.g. to use the MaskedPPO class, you can use ppo as algorithm in the config.

\sphinxAtStartPar
If you add new algorithms, you can extend this dictionary to assign your algorithm class a short identifier.


\begin{fulllineitems}
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.train\_test\_utility\_functions.}}\sphinxbfcode{\sphinxupquote{load\_config}}}{\emph{\DUrole{n}{config\_path}}, \emph{\DUrole{n}{external\_config}}}{{ $\rightarrow$ dict}}
\sphinxAtStartPar
Uses the ConfigHandler routines to load the config according to the path
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config\_path}} \textendash{} Path to the config to be loaded

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{external\_config}} \textendash{} Config dict

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Config

\end{description}\end{quote}

\end{fulllineitems}



\begin{fulllineitems}
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.train\_test\_utility\_functions.}}\sphinxbfcode{\sphinxupquote{load\_data}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}}{{ $\rightarrow$ List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}
\sphinxAtStartPar
Uses the DataHandler routines to load the training config
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Config dict which specifies a dataset

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Dataset (List of instances)

\end{description}\end{quote}

\end{fulllineitems}



\begin{fulllineitems}
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.train\_test\_utility\_functions.}}\sphinxbfcode{\sphinxupquote{complete\_config}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}}{{ $\rightarrow$ dict}}
\sphinxAtStartPar
If optional parameters have not been defined in the configuration, this function adds default values. Also creates
missing directories, if necessary.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} config file

\item[{Returns}] \leavevmode
\sphinxAtStartPar
completed config file

\end{description}\end{quote}

\end{fulllineitems}



\begin{fulllineitems}
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.train\_test\_utility\_functions.}}\sphinxbfcode{\sphinxupquote{get\_agent\_param\_from\_config}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}}{{ $\rightarrow$ str}}
\sphinxAtStartPar
Check if config has TRAIN or TEST algorithm param and get corresponding class string for algorithm from config
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Config for training or testing

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Agent type string (e.g. ‘ppo’)

\end{description}\end{quote}

\end{fulllineitems}



\begin{fulllineitems}
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.train\_test\_utility\_functions.}}\sphinxbfcode{\sphinxupquote{get\_agent\_class\_from\_config}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}}{{ $\rightarrow$ Any}}
\sphinxAtStartPar
Determines and loads the correct agent class type according the config
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Training config

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Agent class type which can be called

\end{description}\end{quote}

\end{fulllineitems}



\subsection{Environments}
\label{\detokenize{environments:environments}}\label{\detokenize{environments::doc}}
\sphinxAtStartPar
Environments are the core of any DRL project since they specify the “game” and interaction logic


\subsubsection{Tetris\_scheduling Environment}
\label{\detokenize{environments:module-environments.env_tetris_scheduling}}\label{\detokenize{environments:tetris-scheduling-environment}}\index{module@\spxentry{module}!environments.env\_tetris\_scheduling@\spxentry{environments.env\_tetris\_scheduling}}\index{environments.env\_tetris\_scheduling@\spxentry{environments.env\_tetris\_scheduling}!module@\spxentry{module}}
\sphinxAtStartPar
This file provides the scheduling environment class Env,
which can be used to load and simulate scheduling\sphinxhyphen{}problem instances.
\index{Env (class in environments.env\_tetris\_scheduling)@\spxentry{Env}\spxextra{class in environments.env\_tetris\_scheduling}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{environments.env\_tetris\_scheduling.}}\sphinxbfcode{\sphinxupquote{Env}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{data}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{gym.core.Env}}

\sphinxAtStartPar
Environment for scheduling optimization.
This class inherits from the base gym environment, so the functions step, reset, \_state\_obs and render
are implemented and can be used by default.

\sphinxAtStartPar
If you want to customize the given rewards, you can adapt the function compute\_reward.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Dictionary with parameters to specify environment attributes

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data}} \textendash{} Scheduling problem to be solved, so a list of instances

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (environments.env\_tetris\_scheduling.Env method)@\spxentry{\_\_init\_\_()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{data}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}}{}
\end{fulllineitems}

\index{reset() (environments.env\_tetris\_scheduling.Env method)@\spxentry{reset()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{{ $\rightarrow$ List\DUrole{p}{{[}}float\DUrole{p}{{]}}}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Resets the episode information trackers

\item {} 
\sphinxAtStartPar
Updates the number of runs

\item {} 
\sphinxAtStartPar
Loads new instance

\end{itemize}
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
First observation by calling the class function self.state\_obs

\end{description}\end{quote}

\end{fulllineitems}

\index{step() (environments.env\_tetris\_scheduling.Env method)@\spxentry{step()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{step}}}{\emph{action: typing.Union{[}int, float{]}, **kwargs) \sphinxhyphen{}\textgreater{} (typing.List{[}float{]}, typing.Any, \textless{}class \textquotesingle{}bool\textquotesingle{}\textgreater{}, typing.Dict}}{}
\sphinxAtStartPar
Step Function
:param action: Action to be performed on the current state of the environment
:return: Observation, reward, done, infos

\end{fulllineitems}

\index{get\_instance\_info() (environments.env\_tetris\_scheduling.Env method)@\spxentry{get\_instance\_info()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.get_instance_info}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_instance\_info}}}{\emph{\DUrole{n}{) \sphinxhyphen{}\textgreater{} (\textless{}class \textquotesingle{}int\textquotesingle{}\textgreater{}}}, \emph{\DUrole{n}{\textless{}class \textquotesingle{}int\textquotesingle{}\textgreater{}}}, \emph{\DUrole{n}{\textless{}class \textquotesingle{}int\textquotesingle{}\textgreater{}}}, \emph{\DUrole{n}{\textless{}class \textquotesingle{}int\textquotesingle{}\textgreater{}}}}{}
\sphinxAtStartPar
Retrieves info about the instance size and configuration from an instance sample
:return: (number of jobs, number of tasks and the maximum runtime) of this datapoint

\end{fulllineitems}

\index{state\_obs (environments.env\_tetris\_scheduling.Env property)@\spxentry{state\_obs}\spxextra{environments.env\_tetris\_scheduling.Env property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.state_obs}}\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{state\_obs}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }List\DUrole{p}{{[}}float\DUrole{p}{{]}}}}}
\sphinxAtStartPar
Transforms state (task state and factory state) to gym obs
Scales the values between 0\sphinxhyphen{}1 and transforms to onehot encoding
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Observation

\end{description}\end{quote}

\end{fulllineitems}

\index{to\_one\_hot() (environments.env\_tetris\_scheduling.Env static method)@\spxentry{to\_one\_hot()}\spxextra{environments.env\_tetris\_scheduling.Env static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.to_one_hot}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{to\_one\_hot}}}{\emph{\DUrole{n}{x}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{max\_size}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}}{{ $\rightarrow$ numpy.array}}
\sphinxAtStartPar
Convert to One Hot encoding
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} \textendash{} Index which value should be 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_size}} \textendash{} Size of the one hot encoding vector

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
One hot encoded vector

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_valid\_job\_action() (environments.env\_tetris\_scheduling.Env static method)@\spxentry{check\_valid\_job\_action()}\spxextra{environments.env\_tetris\_scheduling.Env static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.check_valid_job_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{check\_valid\_job\_action}}}{\emph{\DUrole{n}{job\_action}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.array}}, \emph{\DUrole{n}{job\_mask}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.array}}}{{ $\rightarrow$ bool}}
\sphinxAtStartPar
Check if job action is valid
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{job\_action}} \textendash{} Job action as one hot vector

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{job\_mask}} \textendash{} One hot vector with ones for each valid job

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
True if job\_action is valid, else False

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_selected\_task() (environments.env\_tetris\_scheduling.Env method)@\spxentry{get\_selected\_task()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.get_selected_task}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_selected\_task}}}{\emph{\DUrole{n}{job\_idx}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}}{{ $\rightarrow$ Tuple\DUrole{p}{{[}}int\DUrole{p}{,}\DUrole{w}{  }src.data\_generator.task.Task\DUrole{p}{{]}}}}
\sphinxAtStartPar
Helper Function to get the selected task (next possible task) only by the job index
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{job\_idx}} \textendash{} job index

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Index of the task in the task list and the selected task

\end{description}\end{quote}

\end{fulllineitems}

\index{choose\_machine() (environments.env\_tetris\_scheduling.Env method)@\spxentry{choose\_machine()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.choose_machine}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{choose\_machine}}}{\emph{\DUrole{n}{task}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.data\_generator.task.Task}}}{{ $\rightarrow$ int}}
\sphinxAtStartPar
This function performs the logic, with which the machine is chosen (in the case of the flexible JSSP)
Implemented at the moment: Choose the machine out of the set of possible machines with the earliest possible
start time
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{task}} \textendash{} Task

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Machine on which the task will be scheduled.

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_action\_mask() (environments.env\_tetris\_scheduling.Env method)@\spxentry{get\_action\_mask()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.get_action_mask}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_action\_mask}}}{}{{ $\rightarrow$ numpy.array}}
\sphinxAtStartPar
Get Action mask
It is needed for the heuristics, the machine selection (and the agent, if it is masked).
0 \sphinxhyphen{}\textgreater{} available
1 \sphinxhyphen{}\textgreater{} not available
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Action mask

\end{description}\end{quote}

\end{fulllineitems}

\index{execute\_action() (environments.env\_tetris\_scheduling.Env method)@\spxentry{execute\_action()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.execute_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{execute\_action}}}{\emph{\DUrole{n}{job\_id}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{task}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{src.data\_generator.task.Task}}, \emph{\DUrole{n}{machine\_id}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
This Function executes a valid action
\sphinxhyphen{} set machine
\sphinxhyphen{} update job and task
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{job\_id}} \textendash{} job\_id of the task to be executed

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{task}} \textendash{} Task

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{machine\_id}} \textendash{} ID of the machine on which the task is to be executed

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_reward() (environments.env\_tetris\_scheduling.Env method)@\spxentry{compute\_reward()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.compute_reward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{compute\_reward}}}{}{{ $\rightarrow$ Any}}
\sphinxAtStartPar
Calculates the reward that will later be returned to the agent. Uses the self.reward\_strategy string to
discriminate between different reward strategies. Default is ‘dense\_reward’.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Reward

\end{description}\end{quote}

\end{fulllineitems}

\index{sparse\_makespan\_reward() (environments.env\_tetris\_scheduling.Env method)@\spxentry{sparse\_makespan\_reward()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.sparse_makespan_reward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{sparse\_makespan\_reward}}}{}{{ $\rightarrow$ int}}
\sphinxAtStartPar
Computes the reward based on the final makespan at the end of the episode. Else 0.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
(int) sparse reward

\end{description}\end{quote}

\end{fulllineitems}

\index{mr2\_reward() (environments.env\_tetris\_scheduling.Env method)@\spxentry{mr2\_reward()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.mr2_reward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{mr2\_reward}}}{}{{ $\rightarrow$ Any}}
\sphinxAtStartPar
Computes mr2 reward based on \sphinxurl{https://doi.org/10.1016/j.engappai.2022.104868}
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
mr2 reward

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_done() (environments.env\_tetris\_scheduling.Env method)@\spxentry{check\_done()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.check_done}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{check\_done}}}{}{{ $\rightarrow$ bool}}
\sphinxAtStartPar
Check if all jobs are done
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
True if all jobs are done, else False

\end{description}\end{quote}

\end{fulllineitems}

\index{calculate\_tardiness() (environments.env\_tetris\_scheduling.Env method)@\spxentry{calculate\_tardiness()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.calculate_tardiness}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{calculate\_tardiness}}}{}{{ $\rightarrow$ int}}
\sphinxAtStartPar
Calculates the tardiness of all jobs
(this is the previous was the calc reward function)
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
(int) tardiness of last solution

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_makespan() (environments.env\_tetris\_scheduling.Env method)@\spxentry{get\_makespan()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.get_makespan}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_makespan}}}{}{}
\sphinxAtStartPar
Returns the current makespan (the time the latest of all scheduled tasks finishes)

\end{fulllineitems}

\index{log\_intermediate\_step() (environments.env\_tetris\_scheduling.Env method)@\spxentry{log\_intermediate\_step()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.log_intermediate_step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{log\_intermediate\_step}}}{}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Log Function
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{close() (environments.env\_tetris\_scheduling.Env method)@\spxentry{close()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
\sphinxAtStartPar
This is a relict of using OpenAI Gym API. This is currently unnecessary.

\end{fulllineitems}

\index{seed() (environments.env\_tetris\_scheduling.Env method)@\spxentry{seed()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.seed}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{seed}}}{\emph{\DUrole{n}{seed}\DUrole{o}{=}\DUrole{default_value}{1}}}{}
\sphinxAtStartPar
This is a relict of using OpenAI Gym API.
Currently unnecessary, because the environment is deterministic \sphinxhyphen{}\textgreater{} no seed is used.

\end{fulllineitems}

\index{render() (environments.env\_tetris\_scheduling.Env method)@\spxentry{render()}\spxextra{environments.env\_tetris\_scheduling.Env method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling.Env.render}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{render}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}human\textquotesingle{}}}}{}
\sphinxAtStartPar
Visualizes the current status of the environment
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{mode}} \textendash{} “human”: Displays the gantt chart,
“image”: Returns an image of the gantt chart

\item[{Returns}] \leavevmode
\sphinxAtStartPar
PIL.Image.Image if mode=image, else None

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Tetris\_scheduling\_indirect\_action Environment}
\label{\detokenize{environments:module-environments.env_tetris_scheduling_indirect_action}}\label{\detokenize{environments:tetris-scheduling-indirect-action-environment}}\index{module@\spxentry{module}!environments.env\_tetris\_scheduling\_indirect\_action@\spxentry{environments.env\_tetris\_scheduling\_indirect\_action}}\index{environments.env\_tetris\_scheduling\_indirect\_action@\spxentry{environments.env\_tetris\_scheduling\_indirect\_action}!module@\spxentry{module}}\index{IndirectActionEnv (class in environments.env\_tetris\_scheduling\_indirect\_action)@\spxentry{IndirectActionEnv}\spxextra{class in environments.env\_tetris\_scheduling\_indirect\_action}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling_indirect_action.IndirectActionEnv}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{environments.env\_tetris\_scheduling\_indirect\_action.}}\sphinxbfcode{\sphinxupquote{IndirectActionEnv}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{data}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{src.environments.env\_tetris\_scheduling.Env}}

\sphinxAtStartPar
Scheduling environment for scheduling optimization according to
\sphinxurl{https://www.sciencedirect.com/science/article/pii/S0952197622001130}.

\sphinxAtStartPar
Main differences to the vanilla environment:
\begin{itemize}
\item {} 
\sphinxAtStartPar
ACTION: Indirect action mapping

\item {} 
\sphinxAtStartPar
REWARD: m\sphinxhyphen{}r2 reward (which means we have to train on the same data again and again)

\item {} 
\sphinxAtStartPar
OBSERVATION: observation different (“normalization” looks like division by max to {[}0, 1{]} in paper code). Not every
part makes sense, due to the different interaction logic

\item {} 
\sphinxAtStartPar
INTERACTION LOGIC WARNING:

\item {} 
\sphinxAtStartPar
original paper: time steps are run through, the agent can take as many actions as it wants per time\sphinxhyphen{}step,
but may not schedule into the past.

\item {} 
\sphinxAtStartPar
our adaptation: we still play tetris, meaning that we schedule whole blocks of work at a time

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Dictionary with parameters to specify environment attributes

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{data}} \textendash{} Scheduling problem to be solved, so a list of instances

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv method)@\spxentry{\_\_init\_\_()}\spxextra{environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling_indirect_action.IndirectActionEnv.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{data}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}}{}
\end{fulllineitems}

\index{step() (environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv method)@\spxentry{step()}\spxextra{environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling_indirect_action.IndirectActionEnv.step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{step}}}{\emph{\DUrole{n}{action}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\sphinxAtStartPar
Step Function
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action}} \textendash{} Action to be performed on the current state of the environment

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kwargs}} \textendash{} should include “action\_mode”, because the interaction pattern between heuristics and
the agent are different and need to be processed differently

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Observation, reward, done, infos

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv method)@\spxentry{reset()}\spxextra{environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling_indirect_action.IndirectActionEnv.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{{ $\rightarrow$ numpy.ndarray}}\begin{itemize}
\item {} 
\sphinxAtStartPar
Resets the episode information trackers

\item {} 
\sphinxAtStartPar
Updates the number of runs

\item {} 
\sphinxAtStartPar
Loads new instance

\end{itemize}
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
First observation by calling the class function self.state\_obs

\end{description}\end{quote}

\end{fulllineitems}

\index{state\_obs (environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv property)@\spxentry{state\_obs}\spxextra{environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling_indirect_action.IndirectActionEnv.state_obs}}\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{state\_obs}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }numpy.ndarray}}}
\sphinxAtStartPar
Transforms state (task state and factory state) to gym obs
Scales the values between 0\sphinxhyphen{}1 and transforms to onehot encoding
Confer \sphinxurl{https://www.sciencedirect.com/science/article/pii/S0952197622001130} section 4.2.1
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Observation

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_action\_mask() (environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv method)@\spxentry{get\_action\_mask()}\spxextra{environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling_indirect_action.IndirectActionEnv.get_action_mask}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_action\_mask}}}{}{{ $\rightarrow$ numpy.array}}
\sphinxAtStartPar
Get Action mask
In this environment, we always treat all actions as valid, because the interaction logic accepts it. Note that
we only allow non\sphinxhyphen{}masked algorithms.
The heuristics, however, still need the job mask.
0 \sphinxhyphen{}\textgreater{} available
1 \sphinxhyphen{}\textgreater{} not available
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
\sphinxAtStartPar
Action mask

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_next\_tasks() (environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv method)@\spxentry{get\_next\_tasks()}\spxextra{environments.env\_tetris\_scheduling\_indirect\_action.IndirectActionEnv method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.env_tetris_scheduling_indirect_action.IndirectActionEnv.get_next_tasks}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_next\_tasks}}}{}{}
\sphinxAtStartPar
returns the next tasks that can be scheduled

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Util Functions}
\label{\detokenize{environments:module-environments.environment_loader}}\label{\detokenize{environments:util-functions}}\index{module@\spxentry{module}!environments.environment\_loader@\spxentry{environments.environment\_loader}}\index{environments.environment\_loader@\spxentry{environments.environment\_loader}!module@\spxentry{module}}\index{EnvironmentLoader (class in environments.environment\_loader)@\spxentry{EnvironmentLoader}\spxextra{class in environments.environment\_loader}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.environment_loader.EnvironmentLoader}}\pysigline{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{environments.environment\_loader.}}\sphinxbfcode{\sphinxupquote{EnvironmentLoader}}}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
Loads the right environment as named in the passed config file.
Also checks if the environment is compatible with the chosen algorithm.
\index{load() (environments.environment\_loader.EnvironmentLoader class method)@\spxentry{load()}\spxextra{environments.environment\_loader.EnvironmentLoader class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.environment_loader.EnvironmentLoader.load}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{load}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ Tuple\DUrole{p}{{[}}Any\DUrole{p}{,}\DUrole{w}{  }str\DUrole{p}{{]}}}}
\sphinxAtStartPar
loading function

\end{fulllineitems}

\index{check\_environment\_agent\_compatibility() (environments.environment\_loader.EnvironmentLoader class method)@\spxentry{check\_environment\_agent\_compatibility()}\spxextra{environments.environment\_loader.EnvironmentLoader class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{environments:environments.environment_loader.EnvironmentLoader.check_environment_agent_compatibility}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{check\_environment\_agent\_compatibility}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{env\_name}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}str\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{algo\_name}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}str\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Check if environment and algorithm are compatible. E.g., some environments may depend on action masking.

\end{fulllineitems}


\end{fulllineitems}



\subsection{Visuals Generator}
\label{\detokenize{visuals_generator:visuals-generator}}\label{\detokenize{visuals_generator::doc}}

\subsubsection{Gantt\sphinxhyphen{}Chart Generator}
\label{\detokenize{visuals_generator:module-visuals_generator.gantt_chart}}\label{\detokenize{visuals_generator:gantt-chart-generator}}\index{module@\spxentry{module}!visuals\_generator.gantt\_chart@\spxentry{visuals\_generator.gantt\_chart}}\index{visuals\_generator.gantt\_chart@\spxentry{visuals\_generator.gantt\_chart}!module@\spxentry{module}}
\sphinxAtStartPar
This file provides functions to visualize the current states of a scheduling problems as gantt charts.
Moreover, the generated gantt chart figures can be saved (e.g. as gif).
\index{GanttChartPlotter (class in visuals\_generator.gantt\_chart)@\spxentry{GanttChartPlotter}\spxextra{class in visuals\_generator.gantt\_chart}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{visuals_generator:visuals_generator.gantt_chart.GanttChartPlotter}}\pysigline{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{visuals\_generator.gantt\_chart.}}\sphinxbfcode{\sphinxupquote{GanttChartPlotter}}}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
This class provides functions to visualize the current states of a scheduling problems as gantt charts and
save them as image or gif.
\index{get\_gantt\_chart\_image() (visuals\_generator.gantt\_chart.GanttChartPlotter class method)@\spxentry{get\_gantt\_chart\_image()}\spxextra{visuals\_generator.gantt\_chart.GanttChartPlotter class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{visuals_generator:visuals_generator.gantt_chart.GanttChartPlotter.get_gantt_chart_image}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{get\_gantt\_chart\_image}}}{\emph{\DUrole{n}{tasks}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}, \emph{\DUrole{n}{show\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}, \emph{\DUrole{n}{return\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{True}}, \emph{\DUrole{n}{quality\_dpi}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{100}}, \emph{\DUrole{n}{overall\_makespan}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{overall\_num\_machines}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{overall\_task\_position\_list}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}List\DUrole{p}{{[}}int\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \textless{}module \textquotesingle{}PIL.Image\textquotesingle{} from \textquotesingle{}C:\textbackslash{}\textbackslash{}Anaconda\textbackslash{}\textbackslash{}lib\textbackslash{}\textbackslash{}site\sphinxhyphen{}packages\textbackslash{}\textbackslash{}PIL\textbackslash{}\textbackslash{}Image.py\textquotesingle{}\textgreater{}}}
\sphinxAtStartPar
Can be used to visualize the current state of a scheduling problem as a gantt chart
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tasks}} \textendash{} List of tasks (instance) to be visualized

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{show\_image}} \textendash{} True, if the generated image is to be visualized

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{return\_image}} \textendash{} True if the generated image is to be returned

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{quality\_dpi}} \textendash{} dpi of the generated image

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overall\_makespan}} \textendash{} Makespan of the scheduling problem. Can be None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overall\_num\_machines}} \textendash{} Number of machines available in the scheduling problem. Can be None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overall\_task\_position\_list}} \textendash{} Task position in original list. Can be None

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Gantt chart image

\end{description}\end{quote}

\end{fulllineitems}

\index{save\_gantt\_chart\_image() (visuals\_generator.gantt\_chart.GanttChartPlotter static method)@\spxentry{save\_gantt\_chart\_image()}\spxextra{visuals\_generator.gantt\_chart.GanttChartPlotter static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{visuals_generator:visuals_generator.gantt_chart.GanttChartPlotter.save_gantt_chart_image}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{save\_gantt\_chart\_image}}}{\emph{\DUrole{n}{gantt\_chart}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{PIL.Image.Image}}, \emph{\DUrole{n}{save\_path\_dir}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{pathlib.Path}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{WindowsPath(\textquotesingle{}C:/Users/Constantin WdP/PycharmProjects/alpha\sphinxhyphen{}mes/scheduling\sphinxhyphen{}sandbox/src/visuals\textquotesingle{})}}, \emph{\DUrole{n}{filename}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}gantt\_chart\textquotesingle{}}}, \emph{\DUrole{n}{file\_type}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}png\textquotesingle{}}}}{{ $\rightarrow$ pathlib.Path}}
\sphinxAtStartPar
Saves the input image
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gantt\_chart}} \textendash{} Gantt chart image

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{save\_path\_dir}} \textendash{} Relative path where the image is to be saved

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{filename}} \textendash{} Name under the image is to be saved

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{file\_type}} \textendash{} Suffix with the image is to be saved

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Path of the saved image

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_gantt\_chart\_image\_and\_save() (visuals\_generator.gantt\_chart.GanttChartPlotter class method)@\spxentry{get\_gantt\_chart\_image\_and\_save()}\spxextra{visuals\_generator.gantt\_chart.GanttChartPlotter class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{visuals_generator:visuals_generator.gantt_chart.GanttChartPlotter.get_gantt_chart_image_and_save}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{get\_gantt\_chart\_image\_and\_save}}}{\emph{\DUrole{n}{tasks}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}, \emph{\DUrole{n}{show\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}, \emph{\DUrole{n}{quality\_dpi}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{100}}, \emph{\DUrole{n}{overall\_makespan}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{overall\_num\_machines}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{overall\_task\_position\_list}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}List\DUrole{p}{{[}}int\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{save\_path\_dir}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{pathlib.Path}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{WindowsPath(\textquotesingle{}C:/Users/Constantin WdP/PycharmProjects/alpha\sphinxhyphen{}mes/scheduling\sphinxhyphen{}sandbox/src/visuals\textquotesingle{})}}, \emph{\DUrole{n}{filename}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}gantt\_chart\textquotesingle{}}}, \emph{\DUrole{n}{file\_type}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}png\textquotesingle{}}}}{{ $\rightarrow$ pathlib.Path}}
\sphinxAtStartPar
Initializes the creation and saving of a gantt chart image
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tasks}} \textendash{} List of tasks (instance) to be visualized

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{show\_image}} \textendash{} True, if the generated image is to be visualized

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{quality\_dpi}} \textendash{} dpi of the generated image

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overall\_makespan}} \textendash{} Makespan of the scheduling problem. Can be None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overall\_num\_machines}} \textendash{} Number of machines available in the scheduling problem. Can be None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overall\_task\_position\_list}} \textendash{} Task position in original list. Can be None

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{save\_path\_dir}} \textendash{} Relative path where the image is to be saved

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{filename}} \textendash{} Name under the image is to be saved

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{file\_type}} \textendash{} Suffix with the image is to be saved

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Path of the saved image

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_gantt\_chart\_gif\_and\_save() (visuals\_generator.gantt\_chart.GanttChartPlotter class method)@\spxentry{get\_gantt\_chart\_gif\_and\_save()}\spxextra{visuals\_generator.gantt\_chart.GanttChartPlotter class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{visuals_generator:visuals_generator.gantt_chart.GanttChartPlotter.get_gantt_chart_gif_and_save}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{get\_gantt\_chart\_gif\_and\_save}}}{\emph{\DUrole{n}{tasks}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}, \emph{\DUrole{n}{save\_path\_dir}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{pathlib.Path}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{WindowsPath(\textquotesingle{}C:/Users/Constantin WdP/PycharmProjects/alpha\sphinxhyphen{}mes/scheduling\sphinxhyphen{}sandbox/src/visuals\textquotesingle{})}}, \emph{\DUrole{n}{filename}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}gantt\_chart\textquotesingle{}}}, \emph{\DUrole{n}{save\_intermediate\_images}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}, \emph{\DUrole{n}{quality\_dpi}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{80}}}{{ $\rightarrow$ pathlib.Path}}
\sphinxAtStartPar
Can be used to generate and save a gif of a gantt chart
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tasks}} \textendash{} List of tasks (instance) to be visualized

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{save\_path\_dir}} \textendash{} Relative path where the gif is to be saved

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{filename}} \textendash{} Name under the gif is to be saved

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{save\_intermediate\_images}} \textendash{} True if the intermediate images of the gif creation should be saved

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{quality\_dpi}} \textendash{} dpi of the generated image

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Path of the saved image

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Data Generator}
\label{\detokenize{data_generator:data-generator}}\label{\detokenize{data_generator::doc}}

\subsubsection{Instance Factory}
\label{\detokenize{data_generator:module-data_generator.instance_factory}}\label{\detokenize{data_generator:instance-factory}}\index{module@\spxentry{module}!data\_generator.instance\_factory@\spxentry{data\_generator.instance\_factory}}\index{data\_generator.instance\_factory@\spxentry{data\_generator.instance\_factory}!module@\spxentry{module}}
\sphinxAtStartPar
This file provides functions to generate scheduling problem instances.

\sphinxAtStartPar
Using this file requires a data\_generation config. For example, it is necessary to specify
the type of the scheduling problem.
\index{generate\_instances\_from\_config() (in module data\_generator.instance\_factory)@\spxentry{generate\_instances\_from\_config()}\spxextra{in module data\_generator.instance\_factory}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.instance_factory.generate_instances_from_config}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{data\_generator.instance\_factory.}}\sphinxbfcode{\sphinxupquote{generate\_instances\_from\_config}}}{\emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}, \emph{\DUrole{n}{print\_info}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}
\sphinxAtStartPar
Generates a list of raw scheduling instances according to the console
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Data\_generation config

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{print\_info}} \textendash{} True if the created instances should be output to the console

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
List of raw scheduling problem instances

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_initial\_instance\_solution() (in module data\_generator.instance\_factory)@\spxentry{compute\_initial\_instance\_solution()}\spxextra{in module data\_generator.instance\_factory}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.instance_factory.compute_initial_instance_solution}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{data\_generator.instance\_factory.}}\sphinxbfcode{\sphinxupquote{compute\_initial\_instance\_solution}}}{\emph{\DUrole{n}{instances}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}}{{ $\rightarrow$ List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}
\sphinxAtStartPar
Initializes multiple processes (optional) to generate deadlines for the raw scheduling problem instances
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{instances}} \textendash{} List of raw scheduling problem instances

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Data\_generation config

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
List of scheduling problems instances with set deadlines

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_deadlines() (in module data\_generator.instance\_factory)@\spxentry{generate\_deadlines()}\spxextra{in module data\_generator.instance\_factory}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.instance_factory.generate_deadlines}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{data\_generator.instance\_factory.}}\sphinxbfcode{\sphinxupquote{generate\_deadlines}}}{\emph{\DUrole{n}{instances}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{instance\_with\_dead\_lines}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{make\_span\_list}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}int\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{dict}}}{{ $\rightarrow$ None}}
\sphinxAtStartPar
Generates suitable deadlines for the input instances
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{instances}} \textendash{} List of raw scheduling problem instances

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{instance\_with\_dead\_lines}} \textendash{} manager.list() (Only in Multi\sphinxhyphen{}process case)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{make\_span\_list}} \textendash{} manager.list() (Only in Multi\sphinxhyphen{}process case)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config}} \textendash{} Data\_generation config

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
None

\end{description}\end{quote}

\end{fulllineitems}

\index{main() (in module data\_generator.instance\_factory)@\spxentry{main()}\spxextra{in module data\_generator.instance\_factory}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.instance_factory.main}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{data\_generator.instance\_factory.}}\sphinxbfcode{\sphinxupquote{main}}}{\emph{\DUrole{n}{config\_file\_name}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{external\_config}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\end{fulllineitems}

\index{get\_parser\_args() (in module data\_generator.instance\_factory)@\spxentry{get\_parser\_args()}\spxextra{in module data\_generator.instance\_factory}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.instance_factory.get_parser_args}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{data\_generator.instance\_factory.}}\sphinxbfcode{\sphinxupquote{get\_parser\_args}}}{}{}
\sphinxAtStartPar
Get arguments from command line.

\end{fulllineitems}



\subsubsection{Scheduling Problem Factory}
\label{\detokenize{data_generator:module-data_generator.sp_factory}}\label{\detokenize{data_generator:scheduling-problem-factory}}\index{module@\spxentry{module}!data\_generator.sp\_factory@\spxentry{data\_generator.sp\_factory}}\index{data\_generator.sp\_factory@\spxentry{data\_generator.sp\_factory}!module@\spxentry{module}}
\sphinxAtStartPar
Helper function for the instance generation in instance\_factory.py.
\index{SP (class in data\_generator.sp\_factory)@\spxentry{SP}\spxextra{class in data\_generator.sp\_factory}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.sp_factory.SP}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{data\_generator.sp\_factory.}}\sphinxbfcode{\sphinxupquote{SP}}}{\emph{\DUrole{n}{value}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{enum.Enum}}

\sphinxAtStartPar
An enumeration.
\index{jssp (data\_generator.sp\_factory.SP attribute)@\spxentry{jssp}\spxextra{data\_generator.sp\_factory.SP attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.sp_factory.SP.jssp}}\pysigline{\sphinxbfcode{\sphinxupquote{jssp}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\textquotesingle{}\_generate\_instance\_jssp\textquotesingle{}}}}
\end{fulllineitems}

\index{fjssp (data\_generator.sp\_factory.SP attribute)@\spxentry{fjssp}\spxextra{data\_generator.sp\_factory.SP attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.sp_factory.SP.fjssp}}\pysigline{\sphinxbfcode{\sphinxupquote{fjssp}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\textquotesingle{}\_generate\_instance\_fjssp\textquotesingle{}}}}
\end{fulllineitems}

\index{is\_sp\_type\_implemented() (data\_generator.sp\_factory.SP class method)@\spxentry{is\_sp\_type\_implemented()}\spxextra{data\_generator.sp\_factory.SP class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.sp_factory.SP.is_sp_type_implemented}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{is\_sp\_type\_implemented}}}{\emph{\DUrole{n}{sp\_type}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}\textquotesingle{}}}}{{ $\rightarrow$ bool}}
\end{fulllineitems}

\index{str\_list\_of\_sp\_types\_implemented() (data\_generator.sp\_factory.SP class method)@\spxentry{str\_list\_of\_sp\_types\_implemented()}\spxextra{data\_generator.sp\_factory.SP class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.sp_factory.SP.str_list_of_sp_types_implemented}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{str\_list\_of\_sp\_types\_implemented}}}{}{{ $\rightarrow$ List\DUrole{p}{{[}}str\DUrole{p}{{]}}}}
\end{fulllineitems}


\end{fulllineitems}

\index{SPFactory (class in data\_generator.sp\_factory)@\spxentry{SPFactory}\spxextra{class in data\_generator.sp\_factory}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.sp_factory.SPFactory}}\pysigline{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{data\_generator.sp\_factory.}}\sphinxbfcode{\sphinxupquote{SPFactory}}}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}
\index{generate\_instances() (data\_generator.sp\_factory.SPFactory class method)@\spxentry{generate\_instances()}\spxextra{data\_generator.sp\_factory.SPFactory class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.sp_factory.SPFactory.generate_instances}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{generate\_instances}}}{\emph{\DUrole{n}{num\_jobs}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{2}}, \emph{\DUrole{n}{num\_tasks}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{2}}, \emph{\DUrole{n}{num\_machines}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{2}}, \emph{\DUrole{n}{num\_tools}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{2}}, \emph{\DUrole{n}{num\_instances}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{2}}, \emph{\DUrole{n}{runtimes}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}List\DUrole{p}{{[}}int\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{sp\_type}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}jssp\textquotesingle{}}}, \emph{\DUrole{n}{print\_info}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{bool}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}
\sphinxAtStartPar
Creates a list of instances with random values in the range of the input parameters
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_jobs}} \textendash{} number of jobs generated in an instance

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_tasks}} \textendash{} number of tasks per job generated in an instance

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_machines}} \textendash{} number of machines available

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_tools}} \textendash{} number of tools available

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_instances}} \textendash{} number of instances which are to be generated

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{runtimes}} \textendash{} list of possible runtimes for tasks

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sp\_type}} \textendash{} Scheduling problem type (e.g. “jssp”)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{print\_info}} \textendash{} if True additional info printed to console

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
List of list of Task instances which together form an instance

\end{description}\end{quote}

\end{fulllineitems}

\index{set\_deadlines\_to\_max\_deadline\_per\_job() (data\_generator.sp\_factory.SPFactory class method)@\spxentry{set\_deadlines\_to\_max\_deadline\_per\_job()}\spxextra{data\_generator.sp\_factory.SPFactory class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.sp_factory.SPFactory.set_deadlines_to_max_deadline_per_job}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{set\_deadlines\_to\_max\_deadline\_per\_job}}}{\emph{\DUrole{n}{instances}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}, \emph{\DUrole{n}{num\_jobs}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}}{}
\sphinxAtStartPar
Equals all Task deadlines from one Job according to the one of the last task in the job
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{instances}} \textendash{} List of instances

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_jobs}} \textendash{} Number of jobs in an instance

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
List of instances with equaled job deadlines

\end{description}\end{quote}

\end{fulllineitems}

\index{compute\_and\_set\_hashes() (data\_generator.sp\_factory.SPFactory class method)@\spxentry{compute\_and\_set\_hashes()}\spxextra{data\_generator.sp\_factory.SPFactory class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.sp_factory.SPFactory.compute_and_set_hashes}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{compute\_and\_set\_hashes}}}{\emph{\DUrole{n}{instances}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}\DUrole{p}{{]}}}}}{}
\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Task Class}
\label{\detokenize{data_generator:module-data_generator.task}}\label{\detokenize{data_generator:task-class}}\index{module@\spxentry{module}!data\_generator.task@\spxentry{data\_generator.task}}\index{data\_generator.task@\spxentry{data\_generator.task}!module@\spxentry{module}}
\sphinxAtStartPar
This file provides the Task class.
\index{Task (class in data\_generator.task)@\spxentry{Task}\spxextra{class in data\_generator.task}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.task.Task}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{data\_generator.task.}}\sphinxbfcode{\sphinxupquote{Task}}}{\emph{\DUrole{n}{job\_index}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{task\_index}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{machines}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}List\DUrole{p}{{[}}int\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{tools}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}List\DUrole{p}{{[}}int\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{deadline}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{instance\_hash}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{done}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}bool\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{runtime}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{started}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{finished}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{selected\_machine}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{\_n\_machines}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{\_n\_tools}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{\_feasible\_machine\_from\_instance\_init}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{\_feasible\_order\_index\_from\_instance\_init}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
This class can be used to model tasks of a scheduling problem.
Multiple tasks can be used to model jobs of a scheduling problem.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{job\_index}} \textendash{} index of the job to which multiple tasks belong

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{task\_index}} \textendash{} index of the task within the job (unique within job and gives order of tasks)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{machines}} \textendash{} list of machine indices applicable for the specific task (alternatives \sphinxhyphen{} pick one)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tools}} \textendash{} list of tool indices applicable for the specific task (necessary \sphinxhyphen{} pick all)

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{deadline}} \textendash{} time of deadline for the task

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{instance\_hash}} \textendash{} Individual hash to represent the instance

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{done}} \textendash{} bool to determine status as done

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{runtime}} \textendash{} time left for the task

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{started}} \textendash{} time the task started at in the schedule

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{finished}} \textendash{} time the task finished at in the schedule

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{selected\_machine}} \textendash{} selected machine index from the machines list for the specific schedule

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{\_n\_machines}} \textendash{} number of all available machines in the scheduling problem instance

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{\_n\_tools}} \textendash{} number of all available tools in the scheduling problem instance

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{\_feasible\_machine\_from\_instance\_init}} \textendash{} index of machine in the given instance generated by initial environment
run to generate deadline time

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{\_feasible\_order\_index\_from\_instance\_init}} \textendash{} index of task in the given instance
generated by initial environment run to generate deadline time

\end{itemize}

\end{description}\end{quote}
\index{\_\_init\_\_() (data\_generator.task.Task method)@\spxentry{\_\_init\_\_()}\spxextra{data\_generator.task.Task method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.task.Task.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{job\_index}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{task\_index}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{int}}, \emph{\DUrole{n}{machines}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}List\DUrole{p}{{[}}int\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{tools}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}List\DUrole{p}{{[}}int\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{deadline}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{instance\_hash}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{done}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}bool\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{runtime}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{started}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{finished}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{selected\_machine}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{\_n\_machines}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{\_n\_tools}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{\_feasible\_machine\_from\_instance\_init}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{\_feasible\_order\_index\_from\_instance\_init}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{Optional\DUrole{p}{{[}}int\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\end{fulllineitems}

\index{str\_info() (data\_generator.task.Task method)@\spxentry{str\_info()}\spxextra{data\_generator.task.Task method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{data_generator:data_generator.task.Task.str_info}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{str\_info}}}{}{{ $\rightarrow$ str}}
\end{fulllineitems}


\end{fulllineitems}



\subsection{Solvers}
\label{\detokenize{solvers:solvers}}\label{\detokenize{solvers::doc}}

\subsubsection{Exact solver}
\label{\detokenize{agents.solver:exact-solver}}\label{\detokenize{agents.solver::doc}}

\paragraph{Solver}
\label{\detokenize{agents.solver:module-agents.solver.solver}}\label{\detokenize{agents.solver:solver}}\index{module@\spxentry{module}!agents.solver.solver@\spxentry{agents.solver.solver}}\index{agents.solver.solver@\spxentry{agents.solver.solver}!module@\spxentry{module}}
\sphinxAtStartPar
This Solver solves certain scheduling problems optimally

\sphinxAtStartPar
It can handle:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Classic JSSP

\item {} 
\sphinxAtStartPar
Classic FJSSP

\item {} 
\sphinxAtStartPar
both of the above with and without tool constraints

\item {} 
\sphinxAtStartPar
optimization criteria tardiness and makespan

\end{itemize}
\index{OrToolSolver (class in agents.solver.solver)@\spxentry{OrToolSolver}\spxextra{class in agents.solver.solver}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.solver:agents.solver.solver.OrToolSolver}}\pysigline{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.solver.solver.}}\sphinxbfcode{\sphinxupquote{OrToolSolver}}}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
This class can be used to solve JSSP problems. It can handle:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Classic JSSP

\item {} 
\sphinxAtStartPar
Classic FJSSP

\item {} 
\sphinxAtStartPar
both of the above with and without tool constraints

\item {} 
\sphinxAtStartPar
optimization criteria tardiness and makespan

\end{itemize}

\sphinxAtStartPar
Data needs to be passed in ‘instance format’ and is returned in this format, too.
\index{optimize() (agents.solver.solver.OrToolSolver class method)@\spxentry{optimize()}\spxextra{agents.solver.solver.OrToolSolver class method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.solver:agents.solver.solver.OrToolSolver.optimize}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{classmethod\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{optimize}}}{\emph{\DUrole{n}{instance}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}, \emph{\DUrole{n}{objective}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{str}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}makespan\textquotesingle{}}}}{}
\sphinxAtStartPar
Optimizes the passed instance according to the passed objective.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{instance}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}{\hyperref[\detokenize{data_generator:data_generator.task.Task}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{Task}}}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} The instance as a list of Tasks

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{objective}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} Bbjective to be minimized. May be ‘makespan’ or ‘tardiness’.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
tuple(list{[}Task{]}, float) Solved instance and objective value

\end{description}\end{quote}

\end{fulllineitems}

\index{parse\_instance\_to\_solver\_format() (agents.solver.solver.OrToolSolver static method)@\spxentry{parse\_instance\_to\_solver\_format()}\spxextra{agents.solver.solver.OrToolSolver static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.solver:agents.solver.solver.OrToolSolver.parse_instance_to_solver_format}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{parse\_instance\_to\_solver\_format}}}{\emph{\DUrole{n}{instance}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}}{}
\sphinxAtStartPar
Parses the instance to a processable format.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{instance}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}{\hyperref[\detokenize{data_generator:data_generator.task.Task}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{Task}}}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} The instance as a list of Tasks

\item[{Returns}] \leavevmode
\sphinxAtStartPar
jobs lists with Tuples for every task, including all necessary information for the solver function
machine\_id(s), processing\_time, due\_date(set to 0 for all but last task in job), tool

\item[{Example}] \leavevmode
\sphinxAtStartPar
Job2 = {[}({[}0, 2, 3{]}, 8, 0, 4), ({[}0, 2, 3{]}, 6, 0, 2), …{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{parse\_to\_plottable\_format() (agents.solver.solver.OrToolSolver static method)@\spxentry{parse\_to\_plottable\_format()}\spxextra{agents.solver.solver.OrToolSolver static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.solver:agents.solver.solver.OrToolSolver.parse_to_plottable_format}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{parse\_to\_plottable\_format}}}{\emph{\DUrole{n}{original\_instance}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}, \emph{\DUrole{n}{assigned\_jobs\_by\_solver}}}{}
\sphinxAtStartPar
Reformats the solution into the original instance format to be passed to the gantt chart generator.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{original\_instance}} (\sphinxstyleliteralemphasis{\sphinxupquote{list}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{Tasks}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Original instance in original format

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{assigned\_jobs\_by\_solver}} \textendash{} solution passed by the optimize() function

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
list{[}Tasks{]} plottable solved instance

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{get\_perser\_args() (in module agents.solver.solver)@\spxentry{get\_perser\_args()}\spxextra{in module agents.solver.solver}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.solver:agents.solver.solver.get_perser_args}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.solver.solver.}}\sphinxbfcode{\sphinxupquote{get\_perser\_args}}}{}{}
\sphinxAtStartPar
parse arguments from command line

\end{fulllineitems}

\index{main() (in module agents.solver.solver)@\spxentry{main()}\spxextra{in module agents.solver.solver}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.solver:agents.solver.solver.main}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.solver.solver.}}\sphinxbfcode{\sphinxupquote{main}}}{\emph{\DUrole{n}{instances\_data\_file\_path}}, \emph{\DUrole{n}{solver\_objective}}, \emph{\DUrole{n}{write\_to\_file}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{plot\_gantt\_chart}\DUrole{o}{=}\DUrole{default_value}{False}}}{}
\end{fulllineitems}



\subsubsection{Heuristics}
\label{\detokenize{agents.heuristic:heuristics}}\label{\detokenize{agents.heuristic::doc}}

\paragraph{Heuristic Agents}
\label{\detokenize{agents.heuristic:module-agents.heuristic.heuristic_agent}}\label{\detokenize{agents.heuristic:heuristic-agents}}\index{module@\spxentry{module}!agents.heuristic.heuristic\_agent@\spxentry{agents.heuristic.heuristic\_agent}}\index{agents.heuristic.heuristic\_agent@\spxentry{agents.heuristic.heuristic\_agent}!module@\spxentry{module}}
\sphinxAtStartPar
This module provides the following scheduling heuristics as function:
\begin{itemize}
\item {} 
\sphinxAtStartPar
EDD: earliest due date

\item {} 
\sphinxAtStartPar
SPT: shortest processing time first

\item {} 
\sphinxAtStartPar
MTR: most tasks remaining

\item {} 
\sphinxAtStartPar
LTR: least tasks remaining

\item {} 
\sphinxAtStartPar
Random: random action

\end{itemize}

\sphinxAtStartPar
You can implement additional heuristics in this file by specifying a function that takes a list of tasks and an action
mask and returns the index of the job to be scheduled next.

\sphinxAtStartPar
If you want to call your heuristic via the HeuristicSelectionAgent or edit an existing shortcut,
adapt/extend the task\_selection dict attribute of the HeuristicSelectionAgent class.
\begin{quote}\begin{description}
\item[{Example}] \leavevmode
\end{description}\end{quote}

\sphinxAtStartPar
Add a heuristic that returns zeros (this is not a practical example!)
1. Define the according function

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{return\PYGZus{}0\PYGZus{}heuristic}\PYG{p}{(}\PYG{n}{tasks}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n}{Task}\PYG{p}{]}\PYG{p}{,} \PYG{n}{action\PYGZus{}mask}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{int}\PYG{p}{:}
    \PYG{k}{return} \PYG{l+m+mi}{0}
\end{sphinxVerbatim}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
Add the function to the task\_selection dict within the HeuristicSelectionAgent class:

\end{enumerate}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{task\PYGZus{}selections} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{random\PYGZus{}task}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{EDD}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{edd}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SPT}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{spt}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MTR}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{mtr}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{LTR}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{ltr}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ZERO}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{return\PYGZus{}0\PYGZus{}heuristic}
\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}
\index{get\_active\_task\_dict() (in module agents.heuristic.heuristic\_agent)@\spxentry{get\_active\_task\_dict()}\spxextra{in module agents.heuristic.heuristic\_agent}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.heuristic:agents.heuristic.heuristic_agent.get_active_task_dict}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.heuristic.heuristic\_agent.}}\sphinxbfcode{\sphinxupquote{get\_active\_task\_dict}}}{\emph{\DUrole{n}{tasks}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}}{{ $\rightarrow$ dict}}
\sphinxAtStartPar
Helper function to determining the next unfinished task to be processed for each job
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tasks}} \textendash{} List of task objects, so one instance

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Dictionary containing the next tasks to be processed for each job

\end{description}\end{quote}

\sphinxAtStartPar
Would be an empty dictionary if all tasks were completed

\end{fulllineitems}

\index{edd() (in module agents.heuristic.heuristic\_agent)@\spxentry{edd()}\spxextra{in module agents.heuristic.heuristic\_agent}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.heuristic:agents.heuristic.heuristic_agent.edd}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.heuristic.heuristic\_agent.}}\sphinxbfcode{\sphinxupquote{edd}}}{\emph{\DUrole{n}{tasks}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}, \emph{\DUrole{n}{action\_mask}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.array}}}{{ $\rightarrow$ int}}
\sphinxAtStartPar
EDD: earliest due date. Determines the job with the smallest deadline
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tasks}} \textendash{} List of task objects, so one instance

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_mask}} \textendash{} Action mask from the environment that is to receive the action selected by this heuristic

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Index of the job selected according to the heuristic

\end{description}\end{quote}

\end{fulllineitems}

\index{spt() (in module agents.heuristic.heuristic\_agent)@\spxentry{spt()}\spxextra{in module agents.heuristic.heuristic\_agent}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.heuristic:agents.heuristic.heuristic_agent.spt}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.heuristic.heuristic\_agent.}}\sphinxbfcode{\sphinxupquote{spt}}}{\emph{\DUrole{n}{tasks}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}, \emph{\DUrole{n}{action\_mask}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.array}}}{{ $\rightarrow$ int}}
\sphinxAtStartPar
SPT: shortest processing time first. Determines the job of which the next unfinished task has the lowest runtime
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tasks}} \textendash{} List of task objects, so one instance

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_mask}} \textendash{} Action mask from the environment that is to receive the action selected by this heuristic

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Index of the job selected according to the heuristic

\end{description}\end{quote}

\end{fulllineitems}

\index{mtr() (in module agents.heuristic.heuristic\_agent)@\spxentry{mtr()}\spxextra{in module agents.heuristic.heuristic\_agent}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.heuristic:agents.heuristic.heuristic_agent.mtr}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.heuristic.heuristic\_agent.}}\sphinxbfcode{\sphinxupquote{mtr}}}{\emph{\DUrole{n}{tasks}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}, \emph{\DUrole{n}{action\_mask}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.array}}}{{ $\rightarrow$ int}}
\sphinxAtStartPar
MTR: most tasks remaining. Determines the job with the least completed tasks
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tasks}} \textendash{} List of task objects, so one instance

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_mask}} \textendash{} Action mask from the environment that is to receive the action selected by this heuristic

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Index of the job selected according to the heuristic

\end{description}\end{quote}

\end{fulllineitems}

\index{ltr() (in module agents.heuristic.heuristic\_agent)@\spxentry{ltr()}\spxextra{in module agents.heuristic.heuristic\_agent}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.heuristic:agents.heuristic.heuristic_agent.ltr}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.heuristic.heuristic\_agent.}}\sphinxbfcode{\sphinxupquote{ltr}}}{\emph{\DUrole{n}{tasks}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}, \emph{\DUrole{n}{action\_mask}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.array}}}{{ $\rightarrow$ int}}
\sphinxAtStartPar
LTR: least tasks remaining. Determines the job with the most completed tasks
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tasks}} \textendash{} List of task objects, so one instance

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_mask}} \textendash{} Action mask from the environment that is to receive the action selected by this heuristic

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Index of the job selected according to the heuristic

\end{description}\end{quote}

\end{fulllineitems}

\index{random\_task() (in module agents.heuristic.heuristic\_agent)@\spxentry{random\_task()}\spxextra{in module agents.heuristic.heuristic\_agent}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.heuristic:agents.heuristic.heuristic_agent.random_task}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.heuristic.heuristic\_agent.}}\sphinxbfcode{\sphinxupquote{random\_task}}}{\emph{\DUrole{n}{tasks}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{List\DUrole{p}{{[}}src.data\_generator.task.Task\DUrole{p}{{]}}}}, \emph{\DUrole{n}{action\_mask}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{numpy.array}}}{{ $\rightarrow$ int}}
\sphinxAtStartPar
Returns a random task
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{tasks}} \textendash{} Not needed

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{action\_mask}} \textendash{} Action mask from the environment that is to receive the action selected by this heuristic

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Index of the job selected according to the heuristic

\end{description}\end{quote}

\end{fulllineitems}

\index{choose\_random\_machine() (in module agents.heuristic.heuristic\_agent)@\spxentry{choose\_random\_machine()}\spxextra{in module agents.heuristic.heuristic\_agent}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.heuristic:agents.heuristic.heuristic_agent.choose_random_machine}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.heuristic.heuristic\_agent.}}\sphinxbfcode{\sphinxupquote{choose\_random\_machine}}}{\emph{\DUrole{n}{chosen\_task}}, \emph{\DUrole{n}{machine\_mask}}}{{ $\rightarrow$ int}}
\sphinxAtStartPar
Determines a random machine which is available according to the mask and chosen task. Useful for the FJSSP.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{chosen\_task}} \textendash{} ID of the task that is scheduled on the selected machine

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{machine\_mask}} \textendash{} Machine mask from the environment that is to receive the machine action chosen by this function

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Index of the chosen machine

\end{description}\end{quote}

\end{fulllineitems}

\index{choose\_first\_machine() (in module agents.heuristic.heuristic\_agent)@\spxentry{choose\_first\_machine()}\spxextra{in module agents.heuristic.heuristic\_agent}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.heuristic:agents.heuristic.heuristic_agent.choose_first_machine}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.heuristic.heuristic\_agent.}}\sphinxbfcode{\sphinxupquote{choose\_first\_machine}}}{\emph{\DUrole{n}{chosen\_task}}, \emph{\DUrole{n}{machine\_mask}}}{{ $\rightarrow$ int}}
\sphinxAtStartPar
Determines the first (by index) machine which is available according to the mask and chosen task. Useful for the
FJSSP
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{chosen\_task}} \textendash{} ID of the task that is scheduled on the selected machine

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{machine\_mask}} \textendash{} Machine mask from the environment that is to receive the machine action chosen by this function

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Index of the chosen machine

\end{description}\end{quote}

\end{fulllineitems}

\index{HeuristicSelectionAgent (class in agents.heuristic.heuristic\_agent)@\spxentry{HeuristicSelectionAgent}\spxextra{class in agents.heuristic.heuristic\_agent}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.heuristic:agents.heuristic.heuristic_agent.HeuristicSelectionAgent}}\pysigline{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{agents.heuristic.heuristic\_agent.}}\sphinxbfcode{\sphinxupquote{HeuristicSelectionAgent}}}
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxAtStartPar
This class can be used to get the next task according to the heuristic passed as string abbreviation (e.g. EDD).
If you want to edit a shortcut, or add one for your custom heuristic, adapt/extend the task\_selection dict.
\begin{quote}\begin{description}
\item[{Example}] \leavevmode
\end{description}\end{quote}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{my\PYGZus{}custom\PYGZus{}heuristic}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{\PYGZlt{}}\PYG{n}{function} \PYG{n}{body}\PYG{o}{\PYGZgt{}}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}

\sphinxAtStartPar
or

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{task\PYGZus{}selections} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{random\PYGZus{}task}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{XYZ}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{my\PYGZus{}custom\PYGZus{}heuristic}
    \PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}
\index{\_\_init\_\_() (agents.heuristic.heuristic\_agent.HeuristicSelectionAgent method)@\spxentry{\_\_init\_\_()}\spxextra{agents.heuristic.heuristic\_agent.HeuristicSelectionAgent method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.heuristic:agents.heuristic.heuristic_agent.HeuristicSelectionAgent.__init__}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{}{{ $\rightarrow$ None}}
\end{fulllineitems}


\end{fulllineitems}



\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{a}
\item\relax\sphinxstyleindexentry{agents.heuristic.heuristic\_agent}\sphinxstyleindexpageref{agents.heuristic:\detokenize{module-agents.heuristic.heuristic_agent}}
\item\relax\sphinxstyleindexentry{agents.intermediate\_test}\sphinxstyleindexpageref{agents.reinforcement_learning:\detokenize{module-agents.intermediate_test}}
\item\relax\sphinxstyleindexentry{agents.reinforcement\_learning.dqn}\sphinxstyleindexpageref{agents.reinforcement_learning:\detokenize{module-agents.reinforcement_learning.dqn}}
\item\relax\sphinxstyleindexentry{agents.reinforcement\_learning.ppo}\sphinxstyleindexpageref{agents.reinforcement_learning:\detokenize{module-agents.reinforcement_learning.ppo}}
\item\relax\sphinxstyleindexentry{agents.reinforcement\_learning.ppo\_masked}\sphinxstyleindexpageref{agents.reinforcement_learning:\detokenize{module-agents.reinforcement_learning.ppo_masked}}
\item\relax\sphinxstyleindexentry{agents.solver.solver}\sphinxstyleindexpageref{agents.solver:\detokenize{module-agents.solver.solver}}
\item\relax\sphinxstyleindexentry{agents.test}\sphinxstyleindexpageref{agents.reinforcement_learning:\detokenize{module-agents.test}}
\item\relax\sphinxstyleindexentry{agents.train}\sphinxstyleindexpageref{agents.reinforcement_learning:\detokenize{module-agents.train}}
\indexspace
\bigletter{d}
\item\relax\sphinxstyleindexentry{data\_generator.instance\_factory}\sphinxstyleindexpageref{data_generator:\detokenize{module-data_generator.instance_factory}}
\item\relax\sphinxstyleindexentry{data\_generator.sp\_factory}\sphinxstyleindexpageref{data_generator:\detokenize{module-data_generator.sp_factory}}
\item\relax\sphinxstyleindexentry{data\_generator.task}\sphinxstyleindexpageref{data_generator:\detokenize{module-data_generator.task}}
\indexspace
\bigletter{e}
\item\relax\sphinxstyleindexentry{environments.env\_tetris\_scheduling}\sphinxstyleindexpageref{environments:\detokenize{module-environments.env_tetris_scheduling}}
\item\relax\sphinxstyleindexentry{environments.env\_tetris\_scheduling\_indirect\_action}\sphinxstyleindexpageref{environments:\detokenize{module-environments.env_tetris_scheduling_indirect_action}}
\item\relax\sphinxstyleindexentry{environments.environment\_loader}\sphinxstyleindexpageref{environments:\detokenize{module-environments.environment_loader}}
\indexspace
\bigletter{v}
\item\relax\sphinxstyleindexentry{visuals\_generator.gantt\_chart}\sphinxstyleindexpageref{visuals_generator:\detokenize{module-visuals_generator.gantt_chart}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}